{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル0: 解析環境の自動セットアップ\n",
    "# ==============================================================================\n",
    "# このセルを実行するだけで、解析に必要な全てのライブラリがインストールされます。\n",
    "%pip install -q pandas numpy matplotlib seaborn japanize-matplotlib scipy scikit-learn plotly shap umap-learn statsmodels openpyxl setuptools --upgrade kaleido --upgrade nbformat Jinja2\n",
    "\n",
    "print(\"✅ すべてのライブラリのインストール・確認が完了しました。\")\n",
    "print(\"   > これで、以降のセルを順に実行していくことができます。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2fb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル1: 解析環境のセットアップ\n",
    "# ==============================================================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.spatial.distance import pdist, squareform, euclidean\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from itertools import cycle, combinations\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import base64\n",
    "import shap\n",
    "\n",
    "# ★★★★★★★★★★★★ 追加ライブラリ ★★★★★★★★★★★★\n",
    "import umap\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Matplotlibのスタイルとフォントを設定\n",
    "sns.set(style='whitegrid', font=\"IPAexGothic\", context='talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル2: 【ポートフォリオ版】データの読み込み\n",
    "# ==============================================================================\n",
    "# (Moc様へ：このセルは「生データ.txt」の代わりに、\n",
    "#  scikit-learn内蔵の「ワインデータセット」を読み込みます。\n",
    "#  これは、Moc様が「安全に」ポートフォリオを公開するための差し替えです)\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # 1. 練習用「ワインデータセット」を読み込む\n",
    "    wine = load_wine()\n",
    "\n",
    "    # 2. '生データ.txt' の代わりとなるDataFrame（データ表）を作成\n",
    "    df_original = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "    # 3. 'グループ名.csv' の代わりとなる 'group' 列を作成\n",
    "    # wine.target は [0, 1, 2] という「数値」なので、分かりやすい「文字」の名前に変換します\n",
    "    target_names = {0: '銘柄A', 1: '銘柄B', 2: '銘柄C'}\n",
    "    df_original['group'] = [target_names[t] for t in wine.target]\n",
    "    \n",
    "    # 4. サンプル名を付ける (例: sample_0, sample_1...)\n",
    "    df_original.index = [f\"sample_{i}\" for i in range(len(df_original))]\n",
    "    \n",
    "    print(\"✅ 2.1: ポートフォリオ用の「ワインデータセット」を読み込みました。\")\n",
    "    print(\"   > (生データ.txt と グループ名.csv の読み込みは不要です)\")\n",
    "    \n",
    "    # (Moc様の 'セル2' の「ブランク除去」ロジックは、このデータでは不要なため省略します)\n",
    "    print(\"✅ 2.2: データ読み込み完了\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: ポートフォリオ用データの読み込み中にエラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fff1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル3: 入力・出力パスの設定\n",
    "# ==============================================================================\n",
    "import os\n",
    "\n",
    "print(\"\\n--- 2.0: 入力・出力パスの設定 ---\")\n",
    "\n",
    "# --- ユーザー設定項目 (★ ここを設定 ★) ---\n",
    "peak_annotation_dir = \"ピーク一覧\"\n",
    "output_dir = \"解析結果\" # ★ ベースとなるフォルダ名をここで定義\n",
    "\n",
    "# --- 設定の確認 ---\n",
    "if 'peak_annotation_dir' not in locals() or not peak_annotation_dir:\n",
    "    print(\"   > ❌ 警告: 'peak_annotation_dir' が設定されていません。\")\n",
    "elif os.path.isdir(peak_annotation_dir):\n",
    "    print(f\"   > ✅ 化学情報フォルダが確認されました: {os.path.abspath(peak_annotation_dir)}\")\n",
    "else:\n",
    "    print(f\"   > ❌ ERROR: 指定された化学情報フォルダ '{peak_annotation_dir}' が見つかりません。\")\n",
    "\n",
    "# ★ ベースフォルダの作成\n",
    "os.makedirs(output_dir, exist_ok=True) \n",
    "print(f\"   > ✅ 解析結果のベースフォルダは '{output_dir}' です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル4: タイムスタンプ付き出力フォルダの作成\n",
    "# ==============================================================================\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"\\n--- 2.5: タイムスタンプ付き出力フォルダの作成 ---\")\n",
    "\n",
    "try:\n",
    "    # セル2で定義された 'output_dir' (\"解析結果\") を読み込む\n",
    "    if 'output_dir' not in locals():\n",
    "         base_output_dir = \"解析結果\"\n",
    "         print(f\"   > 警告: 'output_dir' が未定義のため、'{base_output_dir}' をベースにします。\")\n",
    "    else:\n",
    "         # 既にタイムスタンプ付きになっている場合に備えて、ベース名を取得\n",
    "         base_output_dir = os.path.dirname(output_dir) if re.search(r'\\d{8}_\\d{6}$', output_dir) else output_dir\n",
    "\n",
    "    # タイムスタンプの生成\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # ★ 'output_dir' をタイムスタンプ付きのパスで上書き ★\n",
    "    output_dir = os.path.join(base_output_dir, timestamp)\n",
    "    \n",
    "    # タイムスタンプ付きフォルダの作成\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"✅ 解析結果は、タイムスタンプ付きフォルダに保存されます。\")\n",
    "    print(f\"   > 出力先: {os.path.abspath(output_dir)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 出力フォルダの作成中にエラーが発生しました: {e}\")\n",
    "    output_dir = \"output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル5: 化学的推定情報の読み込み\n",
    "# ==============================================================================\n",
    "\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import re # (★クレンジングに使用)\n",
    "import os\n",
    "import unicodedata # ★★★ 半角→全角変換のために追加 ★★★\n",
    "\n",
    "print(\"\\n--- 3.0: 化学的推定情報の読み込み ---\")\n",
    "\n",
    "def extract_candidates(df_candidates, filename_for_error=\"\"):\n",
    "    \"\"\"\n",
    "    化学物質候補のDataFrameを処理し、関連性の高い候補を抽出する関数\n",
    "    (★官能的記述子のクレンジングロジックを追加★)\n",
    "    \"\"\"\n",
    "    expected_columns = [\n",
    "        '保持時間(カラム1)', '保持指標(カラム1)',\n",
    "        '保持時間(カラム2)', '保持指標(カラム2)',\n",
    "        'CAS', '分子式', '名前', '関連性指数', '官能的記述子'\n",
    "    ]\n",
    "    if len(df_candidates.columns) == 9:\n",
    "        df_candidates.columns = expected_columns\n",
    "    elif len(df_candidates.columns) >= 9:\n",
    "        if len(df_candidates.columns) > 9:\n",
    "             pass \n",
    "        df_candidates = df_candidates.iloc[:, :9]\n",
    "        df_candidates.columns = expected_columns\n",
    "    else:\n",
    "        print(f\"   > ❌ ERROR ({filename_for_error}): ファイルの列数が不足しています (9列必要, {len(df_candidates.columns)}列)。このファイルをスキップします。\")\n",
    "        return []\n",
    "    \n",
    "    df_candidates['関連性指数'] = pd.to_numeric(df_candidates['関連性指数'], errors='coerce').fillna(-1)\n",
    "    \n",
    "    # --- (★ ここからが修正点 ★) ---\n",
    "    \n",
    "    # (★ 官能的記述子のクレンジング関数を定義 ★)\n",
    "    def clean_descriptor(text):\n",
    "        \"\"\" 官能的記述子の文字列をクレンジングする \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # (★要望2★) 「半角カタカナ1文字+.」を除去 (例: ｱ.)\n",
    "        # 半角カタカナの正規表現 [ｱ-ﾝ] を使用します\n",
    "        text = re.sub(r'[ｱ-ﾝ]\\.', '', text)\n",
    "        \n",
    "        # ★★★★★★★★★★★★★★★★★ 追加 ★★★★★★★★★★★★★★★★★\n",
    "        # 半角カタカナを全角カタカナに変換します\n",
    "        text = unicodedata.normalize('NFKC', text)\n",
    "        # ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "        \n",
    "        # (★要望1★) 「;」を他の区切り文字（カンマ）に統一\n",
    "        # （後続のセル23がカンマやスペースで分割するため、カンマに置き換えます）\n",
    "        text = text.replace(';', ',')\n",
    "        \n",
    "        return text.strip() # 再度stripして不要な空白を除去\n",
    "\n",
    "    # (★ .apply() を使ってクレンジング関数を適用 ★)\n",
    "    df_candidates['官能的記述子'] = df_candidates['官能的記述子'].fillna('').apply(clean_descriptor)\n",
    "    \n",
    "    # --- (★ 修正点ここまで ★) ---\n",
    "\n",
    "    if df_candidates.empty: return []\n",
    "    \n",
    "    df_sorted = df_candidates.sort_values(by='関連性指数', ascending=False)\n",
    "    selected_df = pd.DataFrame(columns=df_sorted.columns)\n",
    "    # (★ max_relevance のバグ修正: クレンジング後のDFから取得するよう修正 ★)\n",
    "    max_relevance = df_candidates['関連性指数'].max()\n",
    "    \n",
    "    if max_relevance <= 0: return []\n",
    "    \n",
    "    # 抽出ロジック (変更なし)\n",
    "    selected_df = pd.concat([selected_df, df_sorted.head(1)]) \n",
    "    selected_df = pd.concat([selected_df, df_sorted[df_sorted['関連性指数'] >= 90]]) \n",
    "    selected_df = pd.concat([selected_df, df_sorted[df_sorted['関連性指数'] >= (max_relevance - 5)]]) \n",
    "    \n",
    "    has_descriptor = (selected_df['官能的記述子'] != '').any()\n",
    "    if not has_descriptor: \n",
    "        df_with_descriptor = df_sorted[df_sorted['官能的記述子'] != '']\n",
    "        if not df_with_descriptor.empty:\n",
    "            selected_df = pd.concat([selected_df, df_with_descriptor.head(1)])\n",
    "            \n",
    "    selected_df = selected_df[~selected_df.index.duplicated(keep='first')] \n",
    "    \n",
    "    return selected_df.to_dict('records')\n",
    "\n",
    "# --- メイン処理 ---\n",
    "# (メイン処理の部分は変更ありません)\n",
    "peak_annotation_dict = {} \n",
    "\n",
    "try:\n",
    "    if 'peak_annotation_dir' not in locals() or not peak_annotation_dir:\n",
    "        print(\"   > ❌ ERROR: 'peak_annotation_dir' が定義されていません。\")\n",
    "        peak_files = []\n",
    "    else:\n",
    "        peak_files = glob.glob(os.path.join(peak_annotation_dir, \"*.txt\"))\n",
    "\n",
    "    if not peak_files:\n",
    "        if 'peak_annotation_dir' in locals():\n",
    "            print(f\"   > ❌ ERROR: '{peak_annotation_dir}' にピークファイル(*.txt)が見つかりません。\")\n",
    "        print(\"   > 化学情報の読み込みをスキップします。\")\n",
    "    else:\n",
    "        print(f\"   > '{peak_annotation_dir}' から {len(peak_files)} 個のピークファイルを読み込みます...\")\n",
    "\n",
    "        common_read_args = {\n",
    "            'sep': '\\t',\n",
    "            'encoding': 'cp932',\n",
    "            'encoding_errors': 'ignore',\n",
    "            'header': 2,\n",
    "            'on_bad_lines': 'skip',\n",
    "            'usecols': range(2, 11)\n",
    "        }\n",
    "\n",
    "        for f in tqdm(peak_files, desc=\"化学情報を処理中\"):\n",
    "            try:\n",
    "                filename = os.path.basename(f)\n",
    "                match = re.match(r\"(\\d+)-(\\d+)\\.txt\", filename)\n",
    "                \n",
    "                if match:\n",
    "                    peak_num = match.group(1)\n",
    "                    column_num = match.group(2)\n",
    "                    peak_name_key = f\"{peak_num}-{column_num}\"\n",
    "                else:\n",
    "                    continue \n",
    "\n",
    "                df_candidates = pd.read_csv(f, **common_read_args)\n",
    "                selected_candidates_list = extract_candidates(df_candidates, filename_for_error=filename)\n",
    "\n",
    "                if selected_candidates_list:\n",
    "                    peak_annotation_dict[peak_name_key] = selected_candidates_list\n",
    "\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"   > 警告: ファイル '{filename}' に有効なデータ行が見つかりませんでした (スキップ)。\")\n",
    "            except Exception as e:\n",
    "                print(f\"   > ❌ ERROR: ファイル '{filename}' の処理中に予期せぬエラー: {e}\")\n",
    "\n",
    "        print(f\"✅ 化学的推定情報の読み込みが完了しました。\")\n",
    "        print(f\"   > {len(peak_annotation_dict)} / {len(peak_files)} ピーク(カラム別)分の情報が辞書に格納されました。\")\n",
    "\n",
    "        if peak_annotation_dict:\n",
    "            try:\n",
    "                first_key = sorted(peak_annotation_dict.keys(), key=lambda x: (int(x.split('-')[0]), int(x.split('-')[1])))[0]\n",
    "                print(f\"   > 例: {first_key} の候補数 = {len(peak_annotation_dict[first_key])}\")\n",
    "            except Exception as e_sort: \n",
    "                 first_key = list(peak_annotation_dict.keys())[0] \n",
    "                 print(f\"   > 例 (非ソート): {first_key} の候補数 = {len(peak_annotation_dict[first_key])}\")\n",
    "\n",
    "        elif len(peak_files) > 0:\n",
    "            print(\"   > 警告: ピーク情報が1件も辞書に格納されませんでした。ファイル名の形式が '番号-番号.txt' になっているか確認してください。\")\n",
    "\n",
    "except Exception as e_main:\n",
    "    print(f\"❌ 化学情報処理のメインプロセスでエラーが発生しました: {e_main}\")\n",
    "    peak_annotation_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bed126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル6: グループ分け（CSVルールベース・順序保持機能）\n",
    "# ==============================================================================\n",
    "#\n",
    "# ★★★ ポートフォリオ版：このセルは「簡略化」します ★★★\n",
    "#\n",
    "# 理由：\n",
    "# 差し替えた「セル2」で、既に 'group' 列（銘柄A, B, C）を作成したため、\n",
    "# 'グループ名.csv' を読み込む必要がなくなりました。\n",
    "#\n",
    "# ただし、後続のセル（セル9, 10, 11...）で使う\n",
    "# 「master_group_order（グラフの順番）」だけは定義する必要があります。\n",
    "#\n",
    "# ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "\n",
    "try:\n",
    "    if 'df_original' in locals() and 'group' in df_original.columns:\n",
    "        # セル2 で作った 'group' 列から、グループの順番を自動で取得します\n",
    "        master_group_order = df_original['group'].unique().tolist()\n",
    "        print(f\"✅ 2.2: グラフのグループ順序を '{master_group_order}' として記憶しました。\")\n",
    "        \n",
    "        # (Moc様の「未分類サンプル」チェックは、このデータでは不要なため省略します)\n",
    "        print(\"\\n✅ グループ情報は 'セル2' で付与済みです。\")\n",
    "        print(\"   > 検出されたグループ:\")\n",
    "        print(df_original['group'].value_counts())\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n❌ エラー: 'df_original' が見つかりません。セル2を先に実行してください。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ グループ分け中にエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル7: サンプル単位の外れ値除去\n",
    "# ==============================================================================\n",
    "rejected_samples = []\n",
    "for group_name in df_original['group'].unique():\n",
    "    group_df = df_original[df_original['group'] == group_name]\n",
    "    if len(group_df) < 3: continue\n",
    "    features_temp = group_df.drop('group', axis=1)\n",
    "    if not features_temp.isnull().values.any():\n",
    "        pc1_scores = PCA(n_components=1).fit_transform(StandardScaler().fit_transform(features_temp))\n",
    "        z_scores = np.abs((pc1_scores - pc1_scores.mean()) / pc1_scores.std())\n",
    "        outlier_indices = np.where(z_scores > 3.0)[0]\n",
    "        if len(outlier_indices) > 0:\n",
    "            rejected_samples.extend(group_df.index[outlier_indices].tolist())\n",
    "\n",
    "df_sample_cleaned = df_original.drop(index=list(set(rejected_samples)))\n",
    "print(f\"✅ 2.3: {len(set(rejected_samples))}個の異常サンプルを除去しました。\")\n",
    "if rejected_samples:\n",
    "    print(f\"   > 除去されたサンプル: {list(set(rejected_samples))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル8: ピーク単位の棄却検定（Z-score法）\n",
    "# ==============================================================================\n",
    "print(\"✅ 2.3.5: Z-scoreを用いた外れ値の棄却検定を開始...\")\n",
    "df_for_rejection_test = df_sample_cleaned.copy()\n",
    "\n",
    "features_only = df_for_rejection_test.drop('group', axis=1)\n",
    "grouped = features_only.groupby(df_for_rejection_test['group'])\n",
    "group_means = grouped.transform('mean')\n",
    "group_stds = grouped.transform('std')\n",
    "\n",
    "z_scores = ((features_only - group_means) / (group_stds + 1e-9)).abs()\n",
    "outlier_mask = z_scores > 3.0\n",
    "outlier_peaks_info = []\n",
    "if outlier_mask.any().any():\n",
    "    outlier_locations = np.where(outlier_mask)\n",
    "    for row_idx, col_idx in zip(*outlier_locations):\n",
    "        sample_name = outlier_mask.index[row_idx]\n",
    "        peak_name = outlier_mask.columns[col_idx]\n",
    "        outlier_peaks_info.append({\n",
    "            \"Sample\": sample_name, \"Group\": df_for_rejection_test.loc[sample_name, 'group'],\n",
    "            \"Peak\": peak_name, \"Value\": features_only.loc[sample_name, peak_name],\n",
    "            \"Z-score\": z_scores.loc[sample_name, peak_name]\n",
    "        })\n",
    "features_with_nan = features_only.where(~outlier_mask, np.nan)\n",
    "df_cleaned = pd.concat([features_with_nan, df_for_rejection_test['group']], axis=1)\n",
    "\n",
    "if outlier_peaks_info:\n",
    "    print(f\"   > 合計 {len(outlier_peaks_info)} 個のピーク値を外れ値として棄却しました（NaNで置換）。\")\n",
    "else:\n",
    "    print(\"   > 棄却される外れ値はありませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル9: 最終クリーニング（IQR法）と欠損値補完\n",
    "# ==============================================================================\n",
    "features_only_cleaned = df_cleaned.drop('group', axis=1)\n",
    "grouped_cleaned = features_only_cleaned.groupby(df_cleaned['group'])\n",
    "Q1 = grouped_cleaned.transform(lambda x: x.quantile(0.25))\n",
    "Q3 = grouped_cleaned.transform(lambda x: x.quantile(0.75))\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outlier_mask_iqr = (features_only_cleaned < lower_bound) | (features_only_cleaned > upper_bound)\n",
    "features_with_nan_iqr = features_only_cleaned.where(~outlier_mask_iqr, np.nan)\n",
    "df_cleaned_iqr = pd.concat([features_with_nan_iqr, df_cleaned['group']], axis=1)\n",
    "\n",
    "df_analysis = df_cleaned_iqr.copy()\n",
    "peak_columns = df_analysis.drop('group', axis=1).columns\n",
    "for peak in peak_columns:\n",
    "    df_analysis[peak] = df_analysis.groupby('group')[peak].transform(lambda x: x.fillna(x.median()))\n",
    "df_analysis.fillna(0, inplace=True)\n",
    "df_analysis.to_csv(f'{output_dir}/cleaned_data.csv')\n",
    "print(\"✅ 2.4: 外れ値処理と欠損値補完が完了。以降、このクリーンデータを使用します。\")\n",
    "\n",
    "features = df_analysis.drop('group', axis=1)\n",
    "groups = df_analysis['group']\n",
    "features_scaled = StandardScaler().fit_transform(features)\n",
    "\n",
    "if 'master_group_order' in locals():\n",
    "    ordered_groups_for_plotting = [g for g in master_group_order if g in groups.unique()]\n",
    "else:\n",
    "    ordered_groups_for_plotting = sorted(groups.unique())\n",
    "print(f\"   > グラフ描画などで使用する最終的なグループの順序: {ordered_groups_for_plotting}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f591ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル10: 全体構造の可visible化 + LDA棒グラフ \n",
    "# =============================================================================\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def apply_upgraded_layout(fig, title_text):\n",
    "    \"\"\"グラフに統一感のある洗練されたレイアウトを適用する\"\"\"\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f\"<b>{title_text}</b>\",\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': {'size': 22, 'family': \"Arial, sans-serif\"}\n",
    "        },\n",
    "        xaxis=dict(\n",
    "            gridcolor='rgba(230, 230, 230, 0.5)',\n",
    "            zerolinecolor='rgba(200, 200, 200, 0.8)'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            gridcolor='rgba(230, 230, 230, 0.5)',\n",
    "            zerolinecolor='rgba(200, 200, 200, 0.8)'\n",
    "        ),\n",
    "        legend={\n",
    "            'font': {'size': 12},\n",
    "            'title_font': {'size': 14, 'family': \"Arial, sans-serif\", 'color': 'black'},\n",
    "            'bgcolor': 'rgba(255, 255, 255, 0.6)',\n",
    "            'bordercolor': 'rgba(200, 200, 200, 0.5)',\n",
    "            'borderwidth': 1\n",
    "        },\n",
    "        plot_bgcolor='rgba(248, 248, 252, 1)',\n",
    "        paper_bgcolor='white',\n",
    "        font={'family': \"Arial, sans-serif\"},\n",
    "        margin=dict(l=60, r=40, t=80, b=60)\n",
    "    )\n",
    "    # マーカーのデザインを更新\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=11, line=dict(width=1, color='DarkSlateGrey')),\n",
    "        selector=dict(mode='markers')\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def add_confidence_ellipse(fig, df, x_col, y_col, group_col):\n",
    "    \"\"\"信頼楕円（グループの囲い）をパスとして描画する改善版\"\"\"\n",
    "    unique_groups = df[group_col].unique()\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    \n",
    "    for i, group_name in enumerate(unique_groups):\n",
    "        subset = df[df[group_col] == group_name]\n",
    "        if len(subset) < 3:\n",
    "            continue\n",
    "            \n",
    "        center = subset[[x_col, y_col]].mean().values\n",
    "        cov = np.cov(subset[x_col], subset[y_col])\n",
    "        eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "        angle_rad = np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0])\n",
    "        \n",
    "        chi2_val = np.sqrt(chi2.ppf(0.95, 2))\n",
    "        a, b = chi2_val * np.sqrt(eigenvals[0]), chi2_val * np.sqrt(eigenvals[1])\n",
    "\n",
    "        t = np.linspace(0, 2 * np.pi, 100)\n",
    "        ellipse_x, ellipse_y = a * np.cos(t), b * np.sin(t)\n",
    "        \n",
    "        rotated_x = center[0] + ellipse_x * np.cos(angle_rad) - ellipse_y * np.sin(angle_rad)\n",
    "        rotated_y = center[1] + ellipse_x * np.sin(angle_rad) + ellipse_y * np.cos(angle_rad)\n",
    "        \n",
    "        path = \"M \" + \" L \".join(f\"{x},{y}\" for x, y in zip(rotated_x, rotated_y)) + \" Z\"\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        fig.add_shape(type=\"path\", path=path, line_color=color, fillcolor=color, opacity=0.15, layer=\"below\")\n",
    "    return fig\n",
    "\n",
    "print(\"\\n--- 3.1: データセットの全体構造 ---\")\n",
    "\n",
    "# スケーリング (一度だけ実行)\n",
    "features_scaled = StandardScaler().fit_transform(features)\n",
    "\n",
    "# PLS-DA\n",
    "plsda = PLSRegression(n_components=2)\n",
    "plsda_df = pd.DataFrame(plsda.fit(features_scaled, pd.get_dummies(groups)).transform(features_scaled), columns=['第1成分', '第2成分'], index=groups.index)\n",
    "plsda_df['グループ'] = groups\n",
    "fig_plsda = px.scatter(plsda_df, x='第1成分', y='第2成分', color='グループ', symbol='グループ', hover_name=plsda_df.index)\n",
    "fig_plsda = apply_upgraded_layout(fig_plsda, 'サンプル群の全体構造（PLS-DA）')\n",
    "fig_plsda = add_confidence_ellipse(fig_plsda, plsda_df, '第1成分', '第2成分', 'グループ')\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "fig_plsda.write_image(f'{output_dir}/PLS-DAプロット.png', width=1000, height=700, scale=2)\n",
    "print(\"   > PLS-DAプロットを生成しました。\")\n",
    "fig_plsda.show()\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_df = pd.DataFrame(pca.fit_transform(features_scaled), columns=['主成分1', '主成分2'], index=groups.index)\n",
    "pca_df['グループ'] = groups\n",
    "fig_pca = px.scatter(pca_df, x='主成分1', y='主成分2', color='グループ', symbol='グループ', hover_name=pca_df.index,\n",
    "                     labels={'主成分1': f'主成分1 (寄与率: {pca.explained_variance_ratio_[0]:.1%})',\n",
    "                             '主成分2': f'主成分2 (寄与率: {pca.explained_variance_ratio_[1]:.1%})'})\n",
    "fig_pca = apply_upgraded_layout(fig_pca, 'サンプル群の自然な構造（PCA）')\n",
    "fig_pca = add_confidence_ellipse(fig_pca, pca_df, '主成分1', '主成分2', 'グループ')\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "fig_pca.write_image(f'{output_dir}/PCAプロット.png', width=1000, height=700, scale=2)\n",
    "print(\"   > PCAプロットを生成しました。\")\n",
    "fig_pca.show()\n",
    "\n",
    "\n",
    "# LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=min(len(groups.unique())-1, 2))\n",
    "lda_scores = lda.fit_transform(features_scaled, groups)\n",
    "# 寄与率を先に取得\n",
    "lda_variance_ratios = lda.explained_variance_ratio_\n",
    "\n",
    "# 1次元 (2群) の場合の処理\n",
    "if lda_scores.shape[1] == 1:\n",
    "    lda_scores = np.hstack([lda_scores, np.zeros_like(lda_scores)])\n",
    "    lda_cols = ['判別軸1', '判別軸2']\n",
    "    labels_lda = {'判別軸1': f'判別軸1 (寄与率: {lda_variance_ratios[0]:.1%})', '判別軸2': '判別軸2'}\n",
    "else:\n",
    "    lda_cols = ['判別軸1', '判別軸2']\n",
    "    labels_lda = {'判別軸1': f'判別軸1 (寄与率: {lda_variance_ratios[0]:.1%})',\n",
    "                  '判別軸2': f'判別軸2 (寄与率: {lda_variance_ratios[1]:.1%})'}\n",
    "\n",
    "lda_df = pd.DataFrame(lda_scores, columns=lda_cols, index=groups.index)\n",
    "lda_df['グループ'] = groups\n",
    "\n",
    "fig_lda = px.scatter(lda_df, x='判別軸1', y='判別軸2', color='グループ', symbol='グループ', hover_name=lda_df.index, labels=labels_lda)\n",
    "fig_lda = apply_upgraded_layout(fig_lda, 'グループ分離を最大化（LDA）')\n",
    "fig_lda = add_confidence_ellipse(fig_lda, lda_df, '判別軸1', '判別軸2', 'グループ')\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "fig_lda.write_image(f'{output_dir}/LDAプロット.png', width=1000, height=700, scale=2)\n",
    "print(\"   > LDAプロットを生成しました。\")\n",
    "fig_lda.show()\n",
    "\n",
    "\n",
    "# --- ★★★【追加図1: LDA棒グラフ (HTML対応/グラデーション/順序 修正Ver)】★★★\n",
    "print(\"     > LDAモデルに基づき、重要ピークを抽出・可視化しています...\")\n",
    "lda_importance = pd.DataFrame(np.abs(lda.coef_).sum(axis=0), index=features.columns, columns=['LDA Contribution Score']).sort_values('LDA Contribution Score', ascending=False)\n",
    "lda_top10_df = lda_importance.head(10)\n",
    "\n",
    "# ★★★ HTMLレポート用のテーブル文字列を「復活」 ★★★\n",
    "lda_top10_html = lda_top10_df.to_html(classes='table comparison-table', float_format='%.4f')\n",
    "\n",
    "# ★ 順序を修正 (ascending=True)\n",
    "lda_top10_plot_df = lda_top10_df.sort_values('LDA Contribution Score', ascending=True) \n",
    "\n",
    "# ★★★★★★★★★★★★★★★★★ 修正点 ★★★★★★★★★★★★★★★★★\n",
    "# 綺麗なグラデーションにするため、スコアの絶対値ではなく、その順位（ランク）で色付けします。\n",
    "# これにより、値が一つだけ突出していても、均等なグラデーションが適用されます。\n",
    "lda_top10_plot_df['color_rank'] = range(len(lda_top10_plot_df))\n",
    "# ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "\n",
    "# ★ 色を修正 (グラデーション)\n",
    "fig_lda_top10 = px.bar(\n",
    "    lda_top10_plot_df, \n",
    "    x='LDA Contribution Score', \n",
    "    y=lda_top10_plot_df.index, \n",
    "    orientation='h', \n",
    "    color='color_rank', # ★ 修正点: スコアの代わりに順位（ランク）の列を指定\n",
    "    color_continuous_scale='Viridis',\n",
    "    labels={'LDA Contribution Score': '寄与度スコア', 'y': 'ピーク名'} \n",
    ")\n",
    "fig_lda_top10 = apply_upgraded_layout(fig_lda_top10, 'LDA 上位10寄与ピーク')\n",
    "fig_lda_top10.update_layout(\n",
    "    coloraxis_showscale=False,\n",
    "    xaxis_title='寄与度スコア', \n",
    "    yaxis_title='ピーク名',\n",
    "    yaxis_type='category',\n",
    "    plot_bgcolor='rgba(248, 248, 252, 1)'\n",
    ")\n",
    "# (グラデーション無効化の原因だった update_traces を削除済)\n",
    "\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "lda_top10_peaks_path = os.path.join(output_dir, 'LDA_Top10_Peaks.png')\n",
    "fig_lda_top10.write_image(lda_top10_peaks_path, width=1000, height=700, scale=2)\n",
    "fig_lda_top10.show()\n",
    "print(f\"   > {lda_top10_peaks_path} を生成しました。\")\n",
    "# --- ★★★ 追加ロジックここまで ★★★ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cd940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル11: 階層的クラスタリング \n",
    "# =============================================================================\n",
    "import plotly.graph_objects as go\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import numpy as np\n",
    "import plotly.express as px # 色分けのためにimport\n",
    "\n",
    "print(\"\\n--- 3.1.2: 階層的クラスタリングによる系統関係の可視化 (手動描画版) ---\")\n",
    "\n",
    "# --- Step 1: SciPyでクラスタリングとデンドログラムの座標データを計算 ---\n",
    "linked = linkage(features_scaled, method='ward')\n",
    "labels_for_dendrogram = [f\"{g} ({s})\" for g, s in zip(groups, groups.index)]\n",
    "\n",
    "dendro_data = dendrogram(\n",
    "    linked,\n",
    "    orientation='right',\n",
    "    labels=labels_for_dendrogram,\n",
    "    no_plot=True\n",
    ")\n",
    "\n",
    "# --- Step 2: Plotlyのgraph_objectsを使って、手動でプロットを再構築 ---\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(dendro_data['icoord'])):\n",
    "    xs, ys = dendro_data['dcoord'][i], dendro_data['icoord'][i]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=xs, y=ys, mode='lines',\n",
    "        line=dict(color='grey', width=1), hoverinfo='none'\n",
    "    ))\n",
    "\n",
    "# --- Step 3: ラベルとレイアウトを整える ---\n",
    "ordered_labels = dendro_data['ivl']\n",
    "unique_groups_sorted = ordered_groups_for_plotting\n",
    "colors = px.colors.qualitative.Plotly\n",
    "group_colors_map = {group: colors[i % len(colors)] for i, group in enumerate(unique_groups_sorted)}\n",
    "\n",
    "colored_labels = []\n",
    "for label in ordered_labels:\n",
    "    group_name = label.split(' ')[0]\n",
    "    color = group_colors_map.get(group_name, 'black')\n",
    "    colored_labels.append(f'<b style=\"color:{color};\">{label}</b>')\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"<b>サンプル間の系統関係（階層的クラスタリング）</b>\",\n",
    "        'y':0.98, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top',\n",
    "        'font': {'size': 24}\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title='ウォード距離 (クラスタ間の非類似度)',\n",
    "        gridcolor='rgba(230, 230, 230, 0.5)',\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        ticktext=colored_labels,\n",
    "        tickvals=np.arange(5, len(ordered_labels)*10 + 5, 10),\n",
    "        side='left', # ラベルを左側に配置\n",
    "        tickfont=dict(size=12)\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    showlegend=False,\n",
    "    height=max(600, len(ordered_labels) * 25),\n",
    "    yaxis_automargin=True # ラベルに合わせて余白を自動調整\n",
    ")\n",
    "\n",
    "print(\"   > Plotly手動描画版デンドログラムを生成しました。\")\n",
    "fig.show()\n",
    "\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "fig.write_image(f'{output_dir}/階層的クラスタリング.png', width=1200, height=max(600, len(ordered_labels) * 25), scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル12: UMAPによる非線形構造の可視化\n",
    "# =============================================================================\n",
    "print(\"\\n--- 3.1.3: UMAPによる非線形構造の可視化 ---\")\n",
    "\n",
    "# UMAPモデルを定義し、スケール済みデータに適用\n",
    "reducer = umap.UMAP(n_neighbors=max(2, int(len(features) * 0.1)), min_dist=0.1, random_state=42)\n",
    "embedding = reducer.fit_transform(features_scaled)\n",
    "\n",
    "# 結果をデータフレームに格納\n",
    "umap_df = pd.DataFrame(embedding, columns=['UMAP 1', 'UMAP 2'], index=groups.index)\n",
    "umap_df['グループ'] = groups\n",
    "\n",
    "# Plotlyで可視化\n",
    "fig_umap = px.scatter(umap_df, x='UMAP 1', y='UMAP 2', color='グループ', symbol='グループ', hover_name=umap_df.index)\n",
    "# ★レイアウト適用\n",
    "fig_umap = apply_upgraded_layout(fig_umap, 'UMAPによるサンプル分布')\n",
    "fig_umap = add_confidence_ellipse(fig_umap, umap_df, 'UMAP 1', 'UMAP 2', 'グループ')\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "fig_umap.write_image(f'{output_dir}/UMAPプロット.png', width=1000, height=700, scale=2)\n",
    "print(\"   > UMAPプロットを生成しました。\")\n",
    "fig_umap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル13: t-SNEによる非線形可視化\n",
    "# =============================================================================\n",
    "print(\"\\n--- 3.1.4: t-SNEによる非線形構造の可視化 ---\")\n",
    "\n",
    "# サンプル数が少ないため、Perplexityを調整\n",
    "perplexity_value = max(min(5, len(features) - 2), 1)\n",
    "\n",
    "# ★★★★★★★★★★★★ 修正箇所 ★★★★★★★★★★★★\n",
    "# n_iter を max_iter に変更\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42, max_iter=1000)\n",
    "# ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "\n",
    "scores_tsne = tsne.fit_transform(features_scaled)\n",
    "\n",
    "tsne_df = pd.DataFrame(scores_tsne, columns=['t-SNE 1', 't-SNE 2'], index=groups.index)\n",
    "tsne_df['グループ'] = groups\n",
    "\n",
    "fig_tsne = px.scatter(tsne_df, x='t-SNE 1', y='t-SNE 2', color='グループ', symbol='グループ', hover_name=tsne_df.index)\n",
    "# ★レイアウト適用\n",
    "fig_tsne = apply_upgraded_layout(fig_tsne, 't-SNEによるサンプル分布')\n",
    "fig_tsne = add_confidence_ellipse(fig_tsne, tsne_df, 't-SNE 1', 't-SNE 2', 'グループ')\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "fig_tsne.write_image(f'{output_dir}/t-SNEプロット.png', width=1000, height=700, scale=2)\n",
    "print(\"   > t-SNEプロットを生成しました。\")\n",
    "fig_tsne.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル14: 重要特徴量の特定 - \n",
    "# =============================================================================\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- 3.2: 有意差特徴量の特定 ---\")\n",
    "\n",
    "p_values = [f_oneway(*[df_analysis[peak][groups == g] for g in groups.unique()])[1] for peak in features.columns]\n",
    "anova_results_df = pd.DataFrame({'ピーク名': features.columns, 'p値': p_values})\n",
    "p_values_for_correction = np.clip(anova_results_df['p値'].fillna(1.0), 0.0, 1.0)\n",
    "_, q_values, _, _ = multipletests(p_values_for_correction, alpha=0.05, method='fdr_bh')\n",
    "anova_results_df['q値'] = q_values\n",
    "anova_results_df.sort_values(by='p値', inplace=True)\n",
    "print(\"   > ANOVAによるp値、q値の計算が完了しました。\")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42).fit(features, groups)\n",
    "importances_df = pd.DataFrame({'ピーク名': features.columns, '重要度': model.feature_importances_}).sort_values('重要度', ascending=False)\n",
    "print(\"   > ランダムフォレストによる重要度の計算が完了しました。\")\n",
    "\n",
    "merged_results = pd.merge(anova_results_df, importances_df, on='ピーク名')\n",
    "merged_results['総合ランク'] = merged_results['p値'].rank() + merged_results['重要度'].rank(ascending=False)\n",
    "final_results = merged_results.sort_values('総合ランク').reset_index(drop=True)\n",
    "final_results = final_results[['ピーク名', 'p値', 'q値', '重要度', '総合ランク']]\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "final_results.to_csv(f'{output_dir}/重要特徴量ランキング.csv', index=False)\n",
    "print(\"   > 統合ランクが完了しました。\")\n",
    "\n",
    "# 4. 重要度を可視化 (最終レイアウト改善版)\n",
    "plot_data = final_results.head(20).copy() # .copy()を追加してSettingWithCopyWarningを回避\n",
    "\n",
    "plot_data['ピーク名'] = '<b>' + plot_data['ピーク名'].astype(str) + '</b>'\n",
    "\n",
    "fig_importance = px.bar(\n",
    "    plot_data, \n",
    "    x='重要度', \n",
    "    y='ピーク名', \n",
    "    orientation='h',\n",
    "    text='重要度',\n",
    "    # ★改善ポイント: スマートなカラーテーマに変更 (末尾の_rは色の順序を反転)\n",
    "    color='重要度', \n",
    "    color_continuous_scale=px.colors.sequential.Cividis_r\n",
    ")\n",
    "\n",
    "# レイアウトを詳細に設定\n",
    "fig_importance.update_layout(\n",
    "    title={\n",
    "        'text': \"<b>予測に重要な成分 トップ20</b>\",\n",
    "        'y':0.98, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top',\n",
    "        'font': {'size': 24, 'family': 'Arial Black'}\n",
    "    },\n",
    "    xaxis_title=\"ランダムフォレストによる特徴量重要度\",\n",
    "    yaxis_title=None,\n",
    "    yaxis={'categoryorder':'total ascending'},\n",
    "    plot_bgcolor='white',\n",
    "    height=800,\n",
    "    # ★改善ポイント: 左の余白を自動調整\n",
    "    yaxis_automargin=True,\n",
    "    margin=dict(r=50, t=60, b=50), # 右・上・下の余白を調整\n",
    "    coloraxis_colorbar=dict(\n",
    "        title=\"重要度\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# グリッド線やテキストのスタイルを調整\n",
    "fig_importance.update_xaxes(\n",
    "    # ★改善ポイント: 不要な縦グリッド線を削除\n",
    "    showgrid=False, \n",
    "    zerolinecolor='lightgrey'\n",
    ")\n",
    "fig_importance.update_yaxes(showgrid=True, gridcolor='rgba(230, 230, 230, 0.5)')\n",
    "fig_importance.update_traces(\n",
    "    # ★改善ポイント: 数値を太字に変更\n",
    "    texttemplate='<b>%{text:.3f}</b>', \n",
    "    textposition='outside',\n",
    "    marker_line_color='grey',\n",
    "    marker_line_width=1, \n",
    "    opacity=0.9\n",
    ")\n",
    "\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "fig_importance.write_image(f'{output_dir}/重要成分ランキング.png', width=1000, height=800, scale=2)\n",
    "fig_importance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9563c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル15: 上位10ピークによるPCA/PLS-DAプロット\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n",
    "import traceback\n",
    "\n",
    "print(\"\\n--- 3.X: 上位10ピークによるPCA/PLS-DA再解析 ---\")\n",
    "\n",
    "# ★★★ HTMLレポート用の変数を初期化 ★★★\n",
    "top10_peak_table_html = \"\"\n",
    "\n",
    "try:\n",
    "    if 'final_results' in locals() and not final_results.empty:\n",
    "        print(\"     > 総合ランク上位10ピークによる再解析を実行中...\")\n",
    "        top10_df_source = final_results.head(10).copy()\n",
    "        top10_peaks = top10_df_source['ピーク名'].tolist()\n",
    "        \n",
    "        # ★★★★★【HTMLテーブル生成コードを復活】★★★★★\n",
    "        # (これが不足していました)\n",
    "        peak_group_mean = df_analysis.groupby('group')[top10_peaks].mean().T\n",
    "        peak_group_mean.index.name = 'ピーク名'\n",
    "        top10_display_df = pd.merge(top10_df_source, peak_group_mean, on='ピーク名')\n",
    "        \n",
    "        # 'ordered_groups_for_plotting' が存在するか確認 (セル6で定義されているはず)\n",
    "        if 'ordered_groups_for_plotting' not in locals():\n",
    "            print(\"   > [警告] ordered_groups_for_plotting が見つかりません。全グループ列を使用します。\")\n",
    "            ordered_groups_for_plotting = groups.unique().tolist()\n",
    "            \n",
    "        top10_display_df['最も多いグループ'] = top10_display_df[ordered_groups_for_plotting].idxmax(axis=1)\n",
    "        display_columns = ['ピーク名', 'p値', '重要度', '総合ランク'] + ordered_groups_for_plotting + ['最も多いグループ']\n",
    "        \n",
    "        # 存在しない列を除外\n",
    "        display_columns = [col for col in display_columns if col in top10_display_df.columns]\n",
    "        \n",
    "        top10_peak_table_html = top10_display_df[display_columns].to_html(classes='table table-striped', index=False, float_format='%.4f')\n",
    "        print(\"     > 上位10ピークのHTMLテーブル (top10_peak_table_html) を生成しました。\")\n",
    "        # ★★★★★【HTMLテーブル生成コードここまで】★★★★★\n",
    "        \n",
    "        X_top10 = features[top10_peaks]\n",
    "        X_top10_scaled = StandardScaler().fit_transform(X_top10)\n",
    "        \n",
    "        # --- PCA (Top10) ---\n",
    "        pca_top10 = PCA(n_components=2)\n",
    "        scores_pca_top10 = pca_top10.fit_transform(X_top10_scaled)\n",
    "        pca_top10_df = pd.DataFrame(scores_pca_top10, columns=['主成分1', '主成分2'], index=groups.index)\n",
    "        pca_top10_df['グループ'] = groups\n",
    "        \n",
    "        fig_pca_top10_score = px.scatter(pca_top10_df, x='主成分1', y='主成分2', color='グループ', symbol='グループ', hover_name=pca_top10_df.index,\n",
    "                            labels={'主成分1': f'主成分1 (寄与率: {pca_top10.explained_variance_ratio_[0]:.1%})', '主成分2': f'主成分2 (寄与率: {pca_top10.explained_variance_ratio_[1]:.1%})'})\n",
    "        \n",
    "        # ★★★ 正しいレイアウト関数を適用 ★★★\n",
    "        fig_pca_top10_score = apply_upgraded_layout(fig_pca_top10_score, '上位10ピークによるPCAスコアプロット')\n",
    "        fig_pca_top10_score = add_confidence_ellipse(fig_pca_top10_score, pca_top10_df, '主成分1', '主成分2', 'グループ')\n",
    "        \n",
    "        # ★ 修正: ファイル名から通し番号を削除\n",
    "        pca_top10_score_path = os.path.join(output_dir, 'PCA_Top10_Score.png')\n",
    "        fig_pca_top10_score.write_image(pca_top10_score_path, width=1000, height=700, scale=2)\n",
    "        fig_pca_top10_score.show()\n",
    "        print(f\"   > {pca_top10_score_path} (PCAスコア) を生成しました。\")\n",
    "\n",
    "        # --- PCA Loading (Top10) ---\n",
    "        loadings = pca_top10.components_.T\n",
    "        fig_pca_top10_loading = go.Figure()\n",
    "        fig_pca_top10_loading.add_trace(go.Scatter(x=loadings[:, 0], y=loadings[:, 1], mode='markers+text', text=top10_peaks, textposition=\"top center\", marker=dict(color='crimson', size=10, line=dict(width=1, color='black'))))\n",
    "        \n",
    "        fig_pca_top10_loading = apply_upgraded_layout(fig_pca_top10_loading, '上位10ピークによるPCAローディングプロット')\n",
    "        fig_pca_top10_loading.update_layout(xaxis_title='主成分1への寄与度', yaxis_title='主成分2への寄与度')\n",
    "        \n",
    "        # ★ 修正: ファイル名から通し番号を削除\n",
    "        pca_top10_loading_path = os.path.join(output_dir, 'PCA_Top10_Loading.png')\n",
    "        fig_pca_top10_loading.write_image(pca_top10_loading_path, width=1000, height=700, scale=2)\n",
    "        fig_pca_top10_loading.show()\n",
    "        print(f\"   > {pca_top10_loading_path} (PCAローディング) を生成しました。\")\n",
    "\n",
    "        # --- PLS-DA (Top10) ---\n",
    "        if len(groups.unique()) > 1:\n",
    "            plsda_top10 = PLSRegression(n_components=2)\n",
    "            scores_pls_top10 = plsda_top10.fit(X_top10_scaled, pd.get_dummies(groups)).transform(X_top10_scaled)\n",
    "            plsda_top10_df = pd.DataFrame(scores_pls_top10, columns=['成分1', '成分2'], index=groups.index)\n",
    "            plsda_top10_df['グループ'] = groups\n",
    "            exp_var_x = np.var(plsda_top10.x_scores_, axis=0) / np.sum(np.var(X_top10_scaled, axis=0))\n",
    "\n",
    "            fig_plsda_top10_score = px.scatter(plsda_top10_df, x='成分1', y='成分2', color='グループ', symbol='グループ', hover_name=plsda_top10_df.index,\n",
    "                                        labels={'成分1': f'成分1 (寄与率: {exp_var_x[0]:.1%})', '成分2': f'成分2 (寄与率: {exp_var_x[1]:.1%})'})\n",
    "            \n",
    "            # ★★★ 正しいレイアウト関数を適用 ★★★\n",
    "            fig_plsda_top10_score = apply_upgraded_layout(fig_plsda_top10_score, '上位10ピークによるPLS-DAスコアプロット')\n",
    "            fig_plsda_top10_score = add_confidence_ellipse(fig_plsda_top10_score, plsda_top10_df, '成分1', '成分2', 'グループ')\n",
    "            \n",
    "            # ★ 修正: ファイル名から通し番号を削除\n",
    "            plsda_top10_score_path = os.path.join(output_dir, 'PLSDA_Top10_Score.png')\n",
    "            fig_plsda_top10_score.write_image(plsda_top10_score_path, width=1000, height=700, scale=2)\n",
    "            fig_plsda_top10_score.show()\n",
    "            print(f\"   > {plsda_top10_score_path} (PLS-DAスコア) を生成しました。\")\n",
    "\n",
    "            # --- PLS-DA Loading (Top10) ---\n",
    "            loadings_pls = plsda_top10.x_loadings_\n",
    "            fig_plsda_top10_loading = go.Figure()\n",
    "            fig_plsda_top10_loading.add_trace(go.Scatter(x=loadings_pls[:, 0], y=loadings_pls[:, 1], mode='markers+text', text=top10_peaks, textposition=\"top center\", marker=dict(color='mediumblue', size=10, line=dict(width=1, color='black'))))\n",
    "            \n",
    "            fig_plsda_top10_loading = apply_upgraded_layout(fig_plsda_top10_loading, '上位10ピークによるPLS-DAローディングプロット')\n",
    "            fig_plsda_top10_loading.update_layout(xaxis_title='主成分1への寄与度', yaxis_title='主成分2への寄与度')\n",
    "            \n",
    "            # ★ 修正: ファイル名から通し番号を削除\n",
    "            plsda_top10_loading_path = os.path.join(output_dir, 'PLSDA_Top10_Loading.png')\n",
    "            fig_plsda_top10_loading.write_image(plsda_top10_loading_path, width=1000, height=700, scale=2)\n",
    "            fig_plsda_top10_loading.show()\n",
    "            print(f\"   > {plsda_top10_loading_path} (PLS-DAローディング) を生成しました。\")\n",
    "        else:\n",
    "            print(\"     > グループが1つのため、PLS-DA（上位10ピーク）はスキップされました。\")\n",
    "    else:\n",
    "        print(\"   > [警告] 統計解析結果 (final_results) が見つからないため、上位10ピークによる再解析はスキップされました。\")\n",
    "except Exception as e:\n",
    "    print(f\"   > [エラー] 上位10ピークの散布図計算中にエラーが発生しました: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"   > ✅ 上位10ピークによるPCA/PLS-DAの生成が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac560cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル16: 分散分析（ANOVA）結果の可視化\n",
    "# =============================================================================\n",
    "print(\"\\n--- 3.2.1: 分散分析（ANOVA）による有意ピークの確認 ---\")\n",
    "\n",
    "# p値でソートした結果を表示\n",
    "anova_results_sorted = anova_results_df.sort_values('p値').reset_index(drop=True)\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "anova_results_sorted.to_csv(f'{output_dir}/ANOVA結果リスト.csv', index=False)\n",
    "\n",
    "print(\"表: グループ間で統計的に有意な差が見られたピーク（トップ20）\")\n",
    "display(anova_results_sorted.head(20).style.format({\n",
    "    'p値': '{:.2e}',\n",
    "    'q値': '{:.2e}'\n",
    "}).set_caption(\"ANOVA結果トップ20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル17: バイオマーカー候補の検証\n",
    "# =============================================================================\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- 3.3: バイオマーカー候補の検証 (Plotly版) ---\")\n",
    "\n",
    "def create_biomarker_violin_plot_plotly(final_results_df, analysis_df, group_order, output_directory):\n",
    "    if final_results_df is None or final_results_df.empty:\n",
    "        print(\"❌ 警告: 'final_results' が未定義または空のため、プロットをスキップします。\")\n",
    "        return\n",
    "\n",
    "    top_4_peaks = final_results_df.head(4)['ピーク名']\n",
    "    \n",
    "    # ★改善ポイント: p値をタイトルに含めるため、ここで subplot_titles を生成\n",
    "    subplot_titles = []\n",
    "    for peak_name in top_4_peaks:\n",
    "        p_val = final_results_df.loc[final_results_df['ピーク名'] == peak_name, 'p値'].iloc[0]\n",
    "        # HTMLを使って、メインタイトルとサブタイトル（p値）を2行で表示\n",
    "        title_string = (\n",
    "            f\"<b>{peak_name}</b>\"\n",
    "            f\"<br><span style='font-size: 13px; color: #555;'>p-value: {p_val:.2e}</span>\"\n",
    "        )\n",
    "        subplot_titles.append(title_string)\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=2, subplot_titles=subplot_titles, vertical_spacing=0.2)\n",
    "\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    group_colors_map = {group: colors[i % len(colors)] for i, group in enumerate(group_order)}\n",
    "\n",
    "    for i, peak_name in enumerate(top_4_peaks):\n",
    "        row, col = (i // 2) + 1, (i % 2) + 1\n",
    "        \n",
    "        for group_name in group_order:\n",
    "            group_data = analysis_df[analysis_df['group'] == group_name][peak_name]\n",
    "            \n",
    "            # ★改善ポイント: バイオリンの形状を滑らかにするためのバンド幅を計算\n",
    "            # データが少なすぎる場合は計算しない\n",
    "            bandwidth = 0\n",
    "            if len(group_data) > 1:\n",
    "                # Silverman's rule of thumb を簡易的に適用\n",
    "                std_dev = np.std(group_data, ddof=1)\n",
    "                if std_dev > 0:\n",
    "                    bandwidth = .9 * std_dev * (len(group_data)) ** (-1/5.)\n",
    "\n",
    "            fig.add_trace(go.Violin(\n",
    "                x0=group_name,\n",
    "                y=group_data,\n",
    "                name=group_name,\n",
    "                legendgroup=group_name,\n",
    "                showlegend=(i == 0),\n",
    "                # ★改善ポイント: デザインを洗練されたスタイルに変更\n",
    "                box_visible=True,       # 内部に箱ひげ図を表示\n",
    "                meanline_visible=True,  # 平均値を線で表示\n",
    "                points=False,           # 個々の点は非表示にしてスッキリさせる\n",
    "                bandwidth=bandwidth,    # 滑らかさを調整\n",
    "                line=dict(color='black', width=1.5), # 輪郭線を太く\n",
    "                fillcolor=group_colors_map.get(group_name),\n",
    "                opacity=0.8,\n",
    "            ), row=row, col=col)\n",
    "            \n",
    "    # ★改善ポイント: fig.add_annotation は不要になったため削除\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={'text': \"<b>最重要特徴量のグループ間分布比較</b>\", 'y':0.98, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top', 'font': {'size': 28}},\n",
    "        height=1000,\n",
    "        width=1200,\n",
    "        plot_bgcolor='white',\n",
    "        legend_title_text='<b>グループ</b>',\n",
    "        showlegend=True,\n",
    "        violingap=0, # バイオリン間のギャップをなくす\n",
    "        violinmode='overlay'\n",
    "    )\n",
    "    # サブプロットのタイトルのフォントサイズを調整\n",
    "    for annotation in fig.layout.annotations:\n",
    "        annotation.font.size = 20\n",
    "\n",
    "    fig.update_yaxes(title_text=\"<b>強度（面積）</b>\", gridcolor='rgba(230, 230, 230, 0.7)', showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True, tickangle=0)\n",
    "\n",
    "    print(\"   > Plotly版バイオリンプロットを生成しました。\")\n",
    "    fig.show()\n",
    "    \n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    # ★ 修正: ファイル名から通し番号を削除\n",
    "    fig.write_image(f'{output_dir}/最重要特徴量バイオリンプロット.png', scale=2)\n",
    "\n",
    "# --- 関数の実行 ---\n",
    "try:\n",
    "    if 'final_results' not in globals(): final_results = pd.DataFrame()\n",
    "    if 'df_analysis' not in globals(): df_analysis = pd.DataFrame()\n",
    "    if 'ordered_groups_for_plotting' not in globals(): ordered_groups_for_plotting = []\n",
    "    if 'output_dir' not in globals(): output_dir = 'output'\n",
    "        \n",
    "    create_biomarker_violin_plot_plotly(final_results, df_analysis, ordered_groups_for_plotting, output_dir)\n",
    "except Exception as e:\n",
    "    print(f\"プロット作成中に予期せぬエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル18: 各グループのトップピーク可視化\n",
    "# =============================================================================\n",
    "print(\"\\n--- 3.3.1: 各グループのトップピーク可視化 ---\")\n",
    "\n",
    "# 上位30の重要特徴量に絞る\n",
    "top_significant_peaks = final_results.head(30)['ピーク名']\n",
    "\n",
    "for group_name in ordered_groups_for_plotting:\n",
    "    # 対象グループとそれ以外のグループでデータを分割\n",
    "    mean_target = df_analysis[df_analysis['group'] == group_name][top_significant_peaks].mean()\n",
    "    mean_others = df_analysis[df_analysis['group'] != group_name][top_significant_peaks].mean()\n",
    "    \n",
    "    # フォールドチェンジを計算 (対象 / その他)\n",
    "    fold_change = (mean_target + 1e-9) / (mean_others + 1e-9)\n",
    "    \n",
    "    # そのグループで特徴的に高い上位5ピークを抽出\n",
    "    top_5_peaks = fold_change.nlargest(5)\n",
    "    \n",
    "    if not top_5_peaks.empty:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.barplot(x=top_5_peaks.values, y=top_5_peaks.index, palette='viridis')\n",
    "        plt.title(f'グループ「{group_name}」を特徴づけるトップ5ピーク', fontsize=20, pad=20)\n",
    "        plt.xlabel('他のグループ全体に対する平均値の比率 (Fold Change)', fontsize=14)\n",
    "        plt.ylabel('ピーク名', fontsize=14)\n",
    "        plt.grid(axis='x', linestyle='--')\n",
    "        \n",
    "        # ★ 修正: ファイル名から通し番号を削除\n",
    "        filename = f'トップピーク_{group_name}.png'\n",
    "        plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n",
    "        print(f\"   > グループ '{group_name}' のトップピークグラフを生成しました。\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bae987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル19: 分類モデルの性能評価\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from itertools import cycle\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import os\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# --- 日本語フォントの自動設定 ---\n",
    "def setup_japanese_font():\n",
    "    font_path = 'ipaexg.ttf'\n",
    "    if os.path.exists(font_path) and font_path not in [f.fname for f in fm.fontManager.ttflist]:\n",
    "        fm.fontManager.addfont(font_path)\n",
    "    # 常にフォントファミリーを設定\n",
    "    try:\n",
    "        plt.rcParams['font.family'] = 'IPAexGothic'\n",
    "    except Exception as e:\n",
    "        print(f\"フォント設定中に警告: {e}\")\n",
    "\n",
    "setup_japanese_font()\n",
    "\n",
    "print(\"\\n--- 3.4: 分類モデルの構築と性能評価 ---\")\n",
    "\n",
    "# 分類精度レポート\n",
    "y_pred = model.predict(features)\n",
    "report_df = pd.DataFrame(classification_report(groups, y_pred, output_dict=True, labels=model.classes_)).transpose().rename(columns={'precision':'適合率', 'recall':'再現率', 'f1-score':'F1スコア', 'support':'サンプル数'})\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "report_df.to_csv(f'{output_dir}/分類精度レポート.csv')\n",
    "print(\"表: 分類精度レポート\")\n",
    "display(report_df.round(2))\n",
    "\n",
    "fig_cm, axes_cm_roc = plt.subplots(1, 2, figsize=(22, 9))\n",
    "fig_cm.suptitle('分類モデルの性能評価', fontsize=24)\n",
    "sns.heatmap(confusion_matrix(groups, y_pred, labels=model.classes_), annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_, annot_kws={\"size\": 30}, ax=axes_cm_roc[0])\n",
    "axes_cm_roc[0].set_title('混同行列', pad=20)\n",
    "axes_cm_roc[0].set_xlabel('予測されたグループ')\n",
    "axes_cm_roc[0].set_ylabel('実際のグループ')\n",
    "\n",
    "y_bin = label_binarize(groups, classes=model.classes_)\n",
    "y_score = model.predict_proba(features)\n",
    "n_classes = y_bin.shape[1]\n",
    "colors = cycle(sns.color_palette(\"husl\", n_classes))\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_bin[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    axes_cm_roc[1].plot(fpr, tpr, color=color, lw=3, label=f'{model.classes_[i]}群 vs その他 (AUC = {roc_auc:0.2f})')\n",
    "axes_cm_roc[1].plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "axes_cm_roc[1].set_xlim([0.0, 1.0])\n",
    "axes_cm_roc[1].set_ylim([0.0, 1.05])\n",
    "axes_cm_roc[1].set_xlabel('偽陽性率')\n",
    "axes_cm_roc[1].set_ylabel('真陽性率')\n",
    "axes_cm_roc[1].set_title('ROC曲線', pad=20)\n",
    "axes_cm_roc[1].legend(loc=\"lower right\")\n",
    "axes_cm_roc[1].grid(True)\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "plt.savefig(f'{output_dir}/モデル性能評価.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def plot_decision_tree_plotly(tree_model, feature_names, class_names):\n",
    "    \"\"\"Plotlyを使ってインタラクティブな決定木を描画する関数\"\"\"\n",
    "    tree = tree_model.tree_\n",
    "    \n",
    "    positions = {}\n",
    "    def get_node_positions(node_id, x=0.5, y=1, width=0.5):\n",
    "        positions[node_id] = (x, y)\n",
    "        left_child, right_child = tree.children_left[node_id], tree.children_right[node_id]\n",
    "        if left_child != right_child:\n",
    "            get_node_positions(left_child, x - width/2, y - 0.2, width/2)\n",
    "            get_node_positions(right_child, x + width/2, y - 0.2, width/2)\n",
    "    get_node_positions(0)\n",
    "\n",
    "    edge_x, edge_y = [], []\n",
    "    for node_id, pos in positions.items():\n",
    "        if tree.children_left[node_id] in positions:\n",
    "            child_pos = positions[tree.children_left[node_id]]\n",
    "            edge_x.extend([pos[0], child_pos[0], None])\n",
    "            edge_y.extend([pos[1], child_pos[1], None])\n",
    "        if tree.children_right[node_id] in positions:\n",
    "            child_pos = positions[tree.children_right[node_id]]\n",
    "            edge_x.extend([pos[0], child_pos[0], None])\n",
    "            edge_y.extend([pos[1], child_pos[1], None])\n",
    "            \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=edge_x, y=edge_y, mode='lines', line=dict(color='grey', width=1), hoverinfo='none'))\n",
    "\n",
    "    node_x, node_y, annotations = [], [], []\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    \n",
    "    for node_id, pos in positions.items():\n",
    "        node_x.append(pos[0]); node_y.append(pos[1])\n",
    "        value = tree.value[node_id][0]\n",
    "        majority_class_idx = np.argmax(value)\n",
    "        \n",
    "        if tree.children_left[node_id] == tree.children_right[node_id]:\n",
    "            annotation_text = f\"<b>{class_names[majority_class_idx]}</b><br>Gini={tree.impurity[node_id]:.2f}<br>Samples={tree.n_node_samples[node_id]}\"\n",
    "        else:\n",
    "            feature = feature_names[tree.feature[node_id]]\n",
    "            threshold = tree.threshold[node_id]\n",
    "            # Kaleidoエラーを回避するため、特殊文字を使わない\n",
    "            annotation_text = f\"<b>{feature} <= {threshold:.2f}</b><br>Gini={tree.impurity[node_id]:.2f}<br>Samples={tree.n_node_samples[node_id]}\"\n",
    "        \n",
    "        annotations.append(dict(x=pos[0], y=pos[1], text=annotation_text, showarrow=False,\n",
    "            font=dict(size=12, color='white'), align=\"center\",\n",
    "            bgcolor=colors[majority_class_idx % len(colors)], bordercolor='black', borderwidth=1, borderpad=4, opacity=0.9))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=node_x, y=node_y, mode='markers', marker=dict(size=1, color='white'), hoverinfo='none'))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title={'text': \"<b>分類ロジックの可視化（決定木）</b>\", 'y':0.98, 'x':0.5, 'font': {'size': 24}},\n",
    "        showlegend=False,\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[min(positions.values(), key=lambda item: item[1])[1]-0.1, 1.1]),\n",
    "        height=max(600, 200 * tree_model.tree_.max_depth),\n",
    "        plot_bgcolor='rgba(248, 248, 252, 1)',\n",
    "        annotations=annotations)\n",
    "    return fig\n",
    "\n",
    "# --- 決定木のモデル作成と描画 ---\n",
    "tree_model = DecisionTreeClassifier(max_depth=4, random_state=42).fit(features, groups)\n",
    "fig_tree = plot_decision_tree_plotly(tree_model, features.columns, model.classes_)\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "fig_tree.write_image(f'{output_dir}/決定木.png', width=1200, scale=2)\n",
    "fig_tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル20: 交差検証による、より信頼性の高いモデル評価\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 追加分析1: 交差検証（クロスバリデーション）---\")\n",
    "\n",
    "# 1. 各グループのサンプル数を確認\n",
    "group_counts = groups.value_counts()\n",
    "print(f\"   > 各グループのサンプル数:\\n{group_counts}\\n\")\n",
    "\n",
    "# 2. 最小サンプル数に合わせて分割数を自動調整（ただし最低2以上）\n",
    "min_samples = group_counts.min()\n",
    "# 交差検証は最低2分割からなので、min_samplesが1の場合は2に、5より大きい場合は5のままにする\n",
    "n_splits_adjusted = max(2, min(min_samples, 5)) \n",
    "\n",
    "if n_splits_adjusted < 5:\n",
    "    print(f\"   > 警告: 最小サンプル数が5未満のため、分割数を {n_splits_adjusted} に調整しました。\")\n",
    "\n",
    "# 3. 調整した分割数で交差検証を設定\n",
    "cv = StratifiedKFold(n_splits=n_splits_adjusted, shuffle=True, random_state=42)\n",
    "\n",
    "cv_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "scores = cross_val_score(cv_model, features, groups, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(f\"   > {n_splits_adjusted}回のテスト結果（正解率）: {[f'{s:.2%}' for s in scores]}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"   > 平均正解率: {np.mean(scores):.2%} (+/- {np.std(scores) * 2:.2%})\")\n",
    "print(\"-\" * 30)\n",
    "print(\"   > この平均正解率が、モデルが未知のデータに対して発揮すると期待される、より現実的な性能です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# セル21: 予測モデルの実用化デモンストレーション\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 4.0: 予測モデルの実用化デモンストレーション ---\")\n",
    "\n",
    "demonstration_results_html = []\n",
    "def predict_new_sample_for_html(sample_data, group_name):\n",
    "    sample_df = pd.DataFrame([sample_data], columns=features.columns)\n",
    "    prediction = model.predict(sample_df)[0]\n",
    "    probabilities = model.predict_proba(sample_df)\n",
    "    prob_df = pd.DataFrame(probabilities, columns=model.classes_, index=['予測確率']).T.sort_values('予測確率', ascending=False)\n",
    "    \n",
    "    html_output = \"<div class='flex-item'><div class='interpretation' style='height: 100%;'>\"\n",
    "    html_output += f\"<h4 style='margin-top:0;'>検証対象: {group_name}</h4>\"\n",
    "    html_output += f\"<p><b>予測グループ: {prediction}</b></p>\"\n",
    "    html_output += prob_df.style.format('{:.2%}').to_html(classes='table table-striped')\n",
    "    html_output += \"</div></div>\"\n",
    "    return html_output\n",
    "\n",
    "if not groups.empty:\n",
    "    print(\"各グループの平均データを「未知サンプル」として予測モデルを検証し、結果をHTML用に保存します。\")\n",
    "    for group_name in ordered_groups_for_plotting:\n",
    "        unknown_sample_data = df_analysis.loc[df_analysis['group'] == group_name, features.columns].mean().values\n",
    "        if len(unknown_sample_data) == len(features.columns):\n",
    "            result_html = predict_new_sample_for_html(unknown_sample_data, group_name)\n",
    "            demonstration_results_html.append(result_html)\n",
    "    print(\"   > 全ての検証が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル22: グループの距離に関する詳細計算と可視化\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, euclidean\n",
    "from itertools import combinations\n",
    "from IPython.display import display, HTML # HTML表示のために追加\n",
    "\n",
    "print(\"\\n--- 5.0: グループの距離に関する詳細計算 ---\\n\")\n",
    "\n",
    "# 特徴量データとグループ情報を結合\n",
    "df_scaled_with_group = pd.DataFrame(features_scaled, index=groups.index, columns=features.columns)\n",
    "df_scaled_with_group['group'] = groups\n",
    "\n",
    "# 内部距離の計算 (グループ内のサンプル同士の平均距離)\n",
    "internal_distances = {\n",
    "    g: np.mean(pdist(df_scaled_with_group[df_scaled_with_group['group'] == g].drop('group', axis=1).values))\n",
    "    if len(df_scaled_with_group[df_scaled_with_group['group'] == g]) > 1 else 0\n",
    "    for g in ordered_groups_for_plotting\n",
    "}\n",
    "\n",
    "# グループ間距離の計算 (グループの重心間の距離)\n",
    "# ★修正: キーを文字列からタプルに変更\n",
    "centroids = df_scaled_with_group.groupby('group').mean()\n",
    "inter_group_distances = {\n",
    "    (g1, g2): euclidean(centroids.loc[g1], centroids.loc[g2])\n",
    "    for g1, g2 in combinations(ordered_groups_for_plotting, 2)\n",
    "}\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# 1. 内部距離の可視化 (まとまり具合)\n",
    "# ---------------------------------\n",
    "num_groups = len(ordered_groups_for_plotting)\n",
    "fig_height = max(7, num_groups * 0.6)\n",
    "plt.figure(figsize=(10, fig_height))\n",
    "\n",
    "internal_dist_series = pd.Series(internal_distances).sort_values(ascending=False)\n",
    "ax1 = sns.barplot(x=internal_dist_series.values, y=internal_dist_series.index, palette='viridis_r')\n",
    "ax1.set_title('各グループの内部距離（まとまり具合）', fontsize=20, pad=20)\n",
    "ax1.set_xlabel('平均内部距離（小さいほど均一）', fontsize=14)\n",
    "ax1.set_ylabel(None)\n",
    "ax1.tick_params(axis='y', labelsize=12)\n",
    "ax1.grid(axis='x', linestyle='--')\n",
    "\n",
    "# 数値をグラフ上に表示\n",
    "for i, v in enumerate(internal_dist_series.values):\n",
    "    ax1.text(v, i, f' {v:.2f}', color='#333', va='center', fontweight='bold', fontsize=13)\n",
    "\n",
    "ax1.set_xlim(right=internal_dist_series.values.max() * 1.15) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/内部距離.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# 2. グループ間距離の可視化 (離れ具合)\n",
    "# ---------------------------------\n",
    "dist_matrix_inter = pd.DataFrame(index=ordered_groups_for_plotting, columns=ordered_groups_for_plotting, dtype=float).fillna(0)\n",
    "# ★修正: タプルキーを使って距離行列を埋める\n",
    "for (g1, g2), dist in inter_group_distances.items():\n",
    "    dist_matrix_inter.loc[g1, g2] = dist_matrix_inter.loc[g2, g1] = dist\n",
    "\n",
    "fig_size = max(8, num_groups * 1.1)\n",
    "annot_font_size = max(11, 17 - num_groups)\n",
    "\n",
    "plt.figure(figsize=(fig_size, fig_size))\n",
    "sns.heatmap(\n",
    "    dist_matrix_inter,\n",
    "    annot=True,\n",
    "    cmap='viridis_r',\n",
    "    fmt='.2f',\n",
    "    linewidths=.5,\n",
    "    annot_kws={\"size\": annot_font_size, \"weight\": \"bold\"}\n",
    ")\n",
    "plt.title('グループ間の距離（離れ具合）', fontsize=20, pad=20)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/グループ間距離.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "summary_parts = []\n",
    "\n",
    "if inter_group_distances:\n",
    "    # 最も距離が遠いペア\n",
    "    max_dist_pair = max(inter_group_distances, key=inter_group_distances.get)\n",
    "    max_dist_val = inter_group_distances[max_dist_pair]\n",
    "    max_dist_str = f\"'{max_dist_pair[0]}'と'{max_dist_pair[1]}'\"\n",
    "    summary_parts.append(f\"<li><b>離れ具合:</b> ペア <b>{max_dist_str}</b> が最も化学的プロファイルが異なる組み合わせです（グループ間距離: {max_dist_val:.2f}）。</li>\")\n",
    "\n",
    "    # 最も距離が近いペア\n",
    "    min_dist_pair = min(inter_group_distances, key=inter_group_distances.get)\n",
    "    min_dist_val = inter_group_distances[min_dist_pair]\n",
    "    min_dist_str = f\"'{min_dist_pair[0]}'と'{min_dist_pair[1]}'\"\n",
    "    summary_parts.append(f\"<li><b>近さ:</b> ペア <b>{min_dist_str}</b> が最も化学的プロファイルが類似している組み合わせです（グループ間距離: {min_dist_val:.2f}）。</li>\")\n",
    "else:\n",
    "    summary_parts.append(\"<li>グループ間の距離データがありません。</li>\")\n",
    "\n",
    "html_summary = \"\\n\".join(summary_parts)\n",
    "display(HTML(f\"<h3>グループ間距離のサマリー</h3><ul>{html_summary}</ul>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3290eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル23: ボルケーノプロット\n",
    "# =============================================================================\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "print(\"\\n--- 追加分析2: ボルケーノプロット ---\")\n",
    "\n",
    "if len(ordered_groups_for_plotting) >= 2:\n",
    "    for group1, group2 in combinations(ordered_groups_for_plotting, 2):\n",
    "        print(f\"\\n   > ペア '{group1}' vs '{group2}' のボルケーノプロットを計算中...\")\n",
    "        group1_mean = df_analysis[df_analysis['group'] == group1][features.columns].mean()\n",
    "        group2_mean = df_analysis[df_analysis['group'] == group2][features.columns].mean()\n",
    "        log2_fold_change = np.log2((group1_mean + 1e-9) / (group2_mean + 1e-9))\n",
    "        \n",
    "        _, p_values_vec = ttest_ind(df_analysis[df_analysis['group'] == group1][features.columns], df_analysis[df_analysis['group'] == group2][features.columns], equal_var=False, axis=0)\n",
    "        \n",
    "        volcano_df = pd.DataFrame({'log2(FoldChange)': log2_fold_change, 'p値': p_values_vec, '-log10(p値)': -np.log10(p_values_vec)})\n",
    "        volcano_df['有意差'] = '変化なし'\n",
    "        volcano_df.loc[(volcano_df['log2(FoldChange)'] > 1) & (volcano_df['p値'] < 0.05), '有意差'] = f'{group1}で増加'\n",
    "        volcano_df.loc[(volcano_df['log2(FoldChange)'] < -1) & (volcano_df['p値'] < 0.05), '有意差'] = f'{group2}で増加'\n",
    "        \n",
    "        fig_volcano = px.scatter(volcano_df, x='log2(FoldChange)', y='-log10(p値)',\n",
    "                                 hover_name=volcano_df.index, color='有意差',\n",
    "                                 color_discrete_map={'変化なし': 'lightgrey', f'{group1}で増加': 'red', f'{group2}で増加': 'blue'},\n",
    "                                 title=f'<b>ボルケーノプロット ({group1} vs {group2})</b>')\n",
    "        fig_volcano.add_hline(y=-np.log10(0.05), line_dash=\"dash\", line_color=\"black\")\n",
    "        fig_volcano.add_vrect(x0=-1, x1=1, fillcolor=\"grey\", opacity=0.1, line_width=0)\n",
    "        \n",
    "        # ★ 修正: ファイル名から通し番号を削除\n",
    "        filename = f\"ボルケーノプロット_{group1}_vs_{group2}.png\"\n",
    "        fig_volcano.write_image(os.path.join(output_dir, filename), width=1000, height=700)\n",
    "        fig_volcano.show()\n",
    "else:\n",
    "    print(\"   > 警告: 比較グループが2つ未満のため、ボルケーノプロットをスキップします。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル24: 上位特徴量同士の相関ヒートマップ\n",
    "# =============================================================================\n",
    "print(\"\\n--- 追加分析3: 上位特徴量の相関ヒートマップ ---\")\n",
    "\n",
    "top_20_features = final_results.head(20)['ピーク名'].tolist()\n",
    "correlation_matrix = df_analysis[top_20_features].corr()\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)\n",
    "plt.title('重要特徴量トップ20の相関ヒートマップ', fontsize=20, pad=20)\n",
    "# ★ 修正: ファイル名から通し番号を削除\n",
    "plt.savefig(f'{output_dir}/相関ヒートマップ.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル25: SHAPによるモデル解釈 + RF棒グラフ追加\n",
    "# =============================================================================\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import shap\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import traceback # ★★★ 修正1: traceback をインポート ★★★\n",
    "import plotly.express as px # ★★★ RF棒グラフのために追加 ★★★\n",
    "\n",
    "\n",
    "# features_scaled (NumPy配列) を、元の特徴量名を持つDataFrameに変換\n",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=features.columns, index=features.index)\n",
    "\n",
    "# y_testが未定義のため、代わりに全データセットの 'groups' を使用する\n",
    "skip_individual_analysis = False\n",
    "try:\n",
    "    if 'groups' in locals() and not groups.empty:\n",
    "        groups_series = groups\n",
    "        print(\"      - ✅ 'groups' を使用して個別サンプル解析を実行します。\")\n",
    "    else:\n",
    "        print(\"      - ⚠️ 'groups' が見つかりません。個別サンプル解析をスキップします。\")\n",
    "        skip_individual_analysis = True\n",
    "except NameError:\n",
    "    print(\"      - ⚠️ 'groups' が見つかりません。個別サンプル解析をスキップします。\")\n",
    "    skip_individual_analysis = True\n",
    "except Exception as e:\n",
    "    print(f\"      - ⚠️ 'groups' の処理中に予期せぬエラーが発生しました: {e}。個別サンプル解析をスキップします。\")\n",
    "    skip_individual_analysis = True\n",
    "\n",
    "print(\"\\n--- 追加分析4: SHAPによるモデル解釈 ---\")\n",
    "\n",
    "try:\n",
    "    # Explainer と shap_values の計算\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_raw = explainer.shap_values(features_scaled_df.values) \n",
    "    \n",
    "    if not isinstance(shap_values_raw, list):\n",
    "        if hasattr(shap_values_raw, 'ndim') and shap_values_raw.ndim == 3:\n",
    "            shap_values = [shap_values_raw[:, :, i] for i in range(shap_values_raw.shape[2])]\n",
    "        else:\n",
    "            shap_values = [shap_values_raw]\n",
    "    else:\n",
    "        shap_values = shap_values_raw\n",
    "        \n",
    "    if len(shap_values) == 0:\n",
    "        raise ValueError(\"SHAP値が計算されませんでした。\")\n",
    "    \n",
    "    print(f\"    [DEBUG] features_scaled_df.shape: {features_scaled_df.shape}\") \n",
    "    if len(shap_values) > 0:\n",
    "        print(f\"    [DEBUG] shap_values[0].shape: {shap_values[0].shape}\") \n",
    "\n",
    "    # 現在のmatplotlib設定を記憶\n",
    "    original_params = mpl.rcParams.copy() \n",
    "\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 12,\n",
    "        'ytick.labelsize': 10\n",
    "    })\n",
    "\n",
    "    # --- 1. サマリープロットの生成 (各クラス) ---\n",
    "    print(\"    > 各クラス（グループ）ごとにSHAPサマリープロットを生成・保存します...\")\n",
    "    loop_range = min(len(model.classes_), len(shap_values))\n",
    "\n",
    "    for i in range(loop_range):\n",
    "        class_name = model.classes_[i]\n",
    "        num_features_to_display = 15\n",
    "        fig_height = max(6, num_features_to_display / 2.0)\n",
    "        plt.figure(figsize=(10, fig_height))\n",
    "        \n",
    "        shap.summary_plot(\n",
    "            shap_values[i], \n",
    "            features_scaled_df,\n",
    "            show=False, \n",
    "            max_display=num_features_to_display\n",
    "        )\n",
    "        \n",
    "        plt.title(f'SHAP Summary Plot for \\\\\"{class_name}\\\\\"', fontsize=18)\n",
    "        plt.tight_layout()\n",
    "        # ★ 修正: ファイル名から通し番号を削除\n",
    "        filename = f'SHAP_Summary_{class_name}.png'\n",
    "        plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"    > {loop_range}個のSHAPサマリープロットを 'output' フォルダに保存しました。\")\n",
    "\n",
    "\n",
    "    # --- 2. 個別サンプル（ディシジョンプロット）の生成 ---\n",
    "    if not skip_individual_analysis:\n",
    "        print(\"    > 各クラスの最初の5サンプルについてディシジョンプロットを生成・保存します...\")\n",
    "        temp_df = features_scaled_df.copy()\n",
    "        temp_df['label'] = groups_series \n",
    "\n",
    "        for i in range(loop_range):\n",
    "            class_name = model.classes_[i]\n",
    "            class_samples = temp_df[temp_df['label'] == class_name].head(5)\n",
    "            \n",
    "            if class_samples.empty:\n",
    "                print(f\"      - クラス '{class_name}' のサンプルが見つかりませんでした。スキップします。\")\n",
    "                continue\n",
    "                \n",
    "            sample_indices_iloc = [features_scaled_df.index.get_loc(idx) for idx in class_samples.index]\n",
    "\n",
    "            for k, sample_index_iloc in enumerate(sample_indices_iloc):\n",
    "                fig = plt.figure(figsize=(12, 6))\n",
    "                shap.decision_plot(\n",
    "                    explainer.expected_value[i], \n",
    "                    shap_values[i][sample_index_iloc],\n",
    "                    features=features_scaled_df.iloc[sample_index_iloc],\n",
    "                    feature_names=features_scaled_df.columns.tolist(),\n",
    "                    title=f'Decision Plot for Sample (Class: {class_name}, Index: {class_samples.index[k]})',\n",
    "                    show=False\n",
    "                )\n",
    "                # ★ 修正: ファイル名から通し番号を削除\n",
    "                filename = f'SHAP_Decision_{class_name}_Sample{class_samples.index[k]}.png'\n",
    "                plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"      - クラス '{class_name}' の個別サンプル {len(sample_indices_iloc)} 個のプロットを保存しました。\")\n",
    "            \n",
    "        print(f\"    > 個別サンプルのSHAPディシジョンプロットを 'output' フォルダに保存しました。\")\n",
    "    else:\n",
    "        print(\"    > 個別サンプル解析は、'groups' が未定義のためスキップされました。\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ SHAPプロットの生成中にエラーが発生しました: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    if 'original_params' in locals():\n",
    "        mpl.rcParams.update(original_params)\n",
    "        print(\"    > Matplotlibの設定を元に戻しました。\")\n",
    "\n",
    "# --- ★★★ RF棒グラフ (HTML対応/グラデーション/順序 修正Ver) ★★★\n",
    "try:\n",
    "    if 'model' in locals() and hasattr(model, 'feature_importances_'):\n",
    "        print(\"\\n     > RandomForestモデルに基づき、重要ピーク(RF棒グラフ)を抽出・可視化しています...\")\n",
    "        \n",
    "        rf_importance = pd.DataFrame(model.feature_importances_, index=features.columns, columns=['Feature Importance (Proxy)']).sort_values('Feature Importance (Proxy)', ascending=False)\n",
    "        rf_top10_df = rf_importance.head(10)\n",
    "        \n",
    "        # ★★★ HTMLレポート用のテーブル文字列を「復活」 ★★★\n",
    "        rf_top10_html_for_umap_tsne = rf_top10_df.to_html(classes='table comparison-table', float_format='%.4f')\n",
    "        \n",
    "        # ★ 順序を修正 (ascending=True)\n",
    "        rf_top10_plot_df = rf_top10_df.sort_values('Feature Importance (Proxy)', ascending=True) \n",
    "\n",
    "        # ★★★★★★★★★★★★★★★★★ 修正点 ★★★★★★★★★★★★★★★★★\n",
    "        # 綺麗なグラデーションを保証するため、LDAと同様に順位（ランク）で色付けします。\n",
    "        rf_top10_plot_df['color_rank'] = range(len(rf_top10_plot_df))\n",
    "        # ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "\n",
    "        # ★ 色を修正 (グラデーション)\n",
    "        fig_rf_top10 = px.bar(\n",
    "            rf_top10_plot_df, \n",
    "            x='Feature Importance (Proxy)', \n",
    "            y=rf_top10_plot_df.index, \n",
    "            orientation='h', \n",
    "            color='color_rank', # ★ 修正点: スコアの代わりに順位（ランク）の列を指定\n",
    "            color_continuous_scale='Plasma',\n",
    "            labels={'Feature Importance (Proxy)': '重要度 (Feature Importance)', 'y': 'ピーク名'}\n",
    "        )\n",
    "        \n",
    "        fig_rf_top10 = apply_upgraded_layout(fig_rf_top10, 'RF 上位10寄与ピーク (代理指標)')\n",
    "        fig_rf_top10.update_layout(\n",
    "            coloraxis_showscale=False,\n",
    "            xaxis_title='重要度 (Feature Importance)', \n",
    "            yaxis_title='ピーク名',\n",
    "            yaxis_type='category',\n",
    "            plot_bgcolor='rgba(248, 248, 252, 1)'\n",
    "        )\n",
    "        # (グラデーション無効化の原因だった update_traces を削除済)\n",
    "        \n",
    "        # ★ 修正: ファイル名から通し番号を削除\n",
    "        rf_top10_peaks_path = os.path.join(output_dir, 'RF_Top10_Peaks_Proxy.png')\n",
    "        fig_rf_top10.write_image(rf_top10_peaks_path, width=1000, height=700, scale=2)\n",
    "        fig_rf_top10.show()\n",
    "        print(f\"   > {rf_top10_peaks_path} を生成しました。\")\n",
    "    \n",
    "    else:\n",
    "        print(\"   > [警告] 'model' 変数が見つからないか、'feature_importances_' 属性を持たないため、RF上位ピーク棒グラフはスキップされました。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   > [エラー] RF棒グラフの生成中にエラーが発生しました: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"   > ✅ SHAP解析、および RF棒グラフの生成が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260150ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル26: AIによる統合考察の生成 (★グループ間距離のバグ修正版★)\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"\\n--- 6.0: AIによる統合考察の生成 ---\")\n",
    "\n",
    "# --- (ヘルパー関数群: 変更なし) ---\n",
    "def get_best_peak_candidate(peak_name, annotation_dict):\n",
    "    if annotation_dict and peak_name in annotation_dict and annotation_dict[peak_name]:\n",
    "        return annotation_dict[peak_name][0]\n",
    "    return None\n",
    "\n",
    "def format_peak_with_chemical(peak_name, annotation_dict, show_name_only=False):\n",
    "    candidate = get_best_peak_candidate(peak_name, annotation_dict)\n",
    "    if candidate:\n",
    "        chem_name = candidate.get('名前', '不明')\n",
    "        if show_name_only:\n",
    "             return f\"<b>{chem_name}</b> ({peak_name})\"\n",
    "        else:\n",
    "             return f\"{peak_name} (<b>{chem_name}</b>)\"\n",
    "    return peak_name\n",
    "\n",
    "def parse_aroma_descriptors(peak_names_list, annotation_dict):\n",
    "    aroma_counter = Counter()\n",
    "    for peak_name in peak_names_list:\n",
    "        candidate = get_best_peak_candidate(peak_name, annotation_dict)\n",
    "        if candidate:\n",
    "            descriptor_text = candidate.get('官能的記述子', '').strip()\n",
    "            if descriptor_text:\n",
    "                descriptors = re.split(r'[;,\\s、。/]+', descriptor_text)\n",
    "                valid_descriptors = [d.strip() for d in descriptors if d.strip() and d.strip().lower() not in ['none', '']]\n",
    "                aroma_counter.update(valid_descriptors)\n",
    "    return aroma_counter\n",
    "\n",
    "def format_aroma_keywords(aroma_counter, top_n=3):\n",
    "    if not aroma_counter:\n",
    "        return \"（特徴的な香り記述子なし）\"\n",
    "    top_aromas = \"、\".join([f\"<b>{word}</b> ({count}回)\" for word, count in aroma_counter.most_common(top_n)])\n",
    "    return top_aromas\n",
    "# --- (ヘルパー関数群ここまで) ---\n",
    "\n",
    "def generate_ai_interpretation(\n",
    "    groups, internal_distances, inter_group_distances, final_results, \n",
    "    df_analysis, ordered_groups, model, report_df, shap_values, features,\n",
    "    peak_annotation_dict \n",
    "):\n",
    "    \"\"\"\n",
    "    全ての分析結果を統合し、グループごとの多角的な考察を自動生成する関数\n",
    "    (★v4.5 グループ間距離バグ修正版★)\n",
    "    \"\"\"\n",
    "    report_parts = []\n",
    "    unique_groups = ordered_groups\n",
    "    group_profile_summaries = {}\n",
    "    \n",
    "    # --- 総合サマリー (変更なし) ---\n",
    "    report_parts.append(\"<h2>総合サマリー</h2>\")\n",
    "    min_internal_dist_group = min(internal_distances, key=internal_distances.get)\n",
    "    max_internal_dist_group = max(internal_distances, key=internal_distances.get)\n",
    "    \n",
    "    # (★ 修正: 以前のバージョンでここもタプルキーを使うように修正済みでした ★)\n",
    "    max_inter_dist_pair_tuple = max(inter_group_distances, key=inter_group_distances.get)\n",
    "    max_inter_dist_pair_str = f\"{max_inter_dist_pair_tuple[0]} vs {max_inter_dist_pair_tuple[1]}\"\n",
    "    report_parts.append(f\"<p>本分析では、{len(unique_groups)}個のグループが特定されました。中でも、グループ<b>'{min_internal_dist_group}'</b>は最も均一性が高く（内部距離: {internal_distances[min_internal_dist_group]:.2f}）、グループ<b>'{max_internal_dist_group}'</b>は最も多様性に富んだ集団であることが示唆されます。また、グループ間の特徴が最も異なっていたのは<b>'{max_inter_dist_pair_str}'</b>のペアでした（グループ間距離: {inter_group_distances[max_inter_dist_pair_tuple]:.2f}）。</p>\")\n",
    "\n",
    "    # --- AIによる重要香気成分の分析 (変更なし) ---\n",
    "    report_parts.append(\"<h3>AIによる重要香気成分の分析 (トップ20)</h3>\")\n",
    "    corr_df = pd.DataFrame() \n",
    "    try:\n",
    "        top_20_peak_names = final_results.head(20)['ピーク名'].tolist()\n",
    "        \n",
    "        # 1. 香りの重要度ランキング\n",
    "        report_parts.append(\"<p><b>1. 全体の香りに寄与する主要キーワード (トップ20ピークより):</b></p>\")\n",
    "        top_20_aromas_counter = parse_aroma_descriptors(top_20_peak_names, peak_annotation_dict)\n",
    "        if not top_20_aromas_counter:\n",
    "            report_parts.append(\"<p style='margin-left: 20px;'>トップ20の重要ピークからは、特徴的な香り記述子は見つかりませんでした。</p>\")\n",
    "        else:\n",
    "            aroma_rank_df = pd.DataFrame(top_20_aromas_counter.most_common(5), columns=['香りキーワード', '出現回数'])\n",
    "            aroma_rank_df.index = aroma_rank_df.index + 1\n",
    "            aroma_rank_df.index.name = \"順位\"\n",
    "            report_parts.append(f\"<div class='table-wrapper'>{aroma_rank_df.to_html(classes='table table-striped', border=0, justify='left')}</div>\")\n",
    "            top_aromas_str = format_aroma_keywords(top_20_aromas_counter, top_n=3)\n",
    "            report_parts.append(f\"<p style='margin-left: 10px; margin-top: 10px;'><b>[AI考察]</b>: モデルが重要と判断したトップ20のピーク全体では、 <b>{top_aromas_str}</b> といった香りが、サンプル間の違いを生み出す主要な要素となっているようです。</p>\")\n",
    "\n",
    "        # 2. 香りの相関関係\n",
    "        report_parts.append(\"<p style='margin-top: 20px;'><b>2. 強く相関する「香りのペア」 (トップ20ピーク内):</b></p>\")\n",
    "        corr_matrix = features[top_20_peak_names].corr()\n",
    "        corr_pairs = corr_matrix.unstack().sort_values(ascending=False)\n",
    "        corr_pairs = corr_pairs[corr_pairs < 0.999]\n",
    "        top_corr_list = []\n",
    "        _done = set()\n",
    "        for (peak_a, peak_b), corr_val in corr_pairs.items():\n",
    "            if (peak_b, peak_a) in _done or corr_val < 0.8: continue\n",
    "            _done.add((peak_a, peak_b))\n",
    "            candidate_a = get_best_peak_candidate(peak_a, peak_annotation_dict)\n",
    "            candidate_b = get_best_peak_candidate(peak_b, peak_annotation_dict)\n",
    "            desc_a = candidate_a.get('官能的記述子', '').strip() if candidate_a else \"\"\n",
    "            desc_b = candidate_b.get('官能的記述子', '').strip() if candidate_b else \"\"\n",
    "            if not desc_a or not desc_b: continue\n",
    "            chem_a_str = format_peak_with_chemical(peak_a, peak_annotation_dict, show_name_only=True)\n",
    "            chem_b_str = format_peak_with_chemical(peak_b, peak_annotation_dict, show_name_only=True)\n",
    "            top_corr_list.append({\n",
    "                '相関係数 (r)': f\"{corr_val:.2f}\",\n",
    "                '香りのペア (A)': f\"{chem_a_str} (<b>{desc_a}</b>)\",\n",
    "                '香りのペア (B)': f\"{chem_b_str} (<b>{desc_b}</b>)\"\n",
    "            })\n",
    "            if len(top_corr_list) >= 20: break\n",
    "                \n",
    "        if not top_corr_list:\n",
    "            report_parts.append(\"<p style='margin-left: 20px;'>重要ピーク間（トップ20）で、両方とも香り情報を持つ強い相関（r > 0.8）のペアは見つかりませんでした。</p>\")\n",
    "        else:\n",
    "            report_parts.append(\"<p>以下は、セットで増減する傾向が強い（r > 0.8）「香りのペア」のリスト（最大20件）です。</p>\")\n",
    "            corr_df = pd.DataFrame(top_corr_list)\n",
    "            corr_df.index = corr_df.index + 1\n",
    "            corr_df.index.name = \"No.\"\n",
    "            report_parts.append(f\"<div class='table-wrapper'>{corr_df.to_html(classes='table table-striped', border=0, justify='left', escape=False)}</div>\")\n",
    "            report_parts.append(\"<p style='margin-left: 10px; margin-top: 10px;'><b>[AI考察]</b>: これらのピーク（香り）はセットで増減する傾向があることを示しています。例えば、一方の香りが存在する場合、もう一方の香りも同様に存在している可能性が非常に高く、これらが組み合わさってサンプル固有の「香りのシグネチャ」を形成していると考えられます。</p>\")\n",
    "\n",
    "    except Exception as e:\n",
    "        report_parts.append(f\"<p style='color:orange;'>重要香気成分の分析中にエラーが発生しました: {e}</p>\")\n",
    "\n",
    "    # --- トップ10ピークによる主成分分析(PCA) (変更なし) ---\n",
    "    report_parts.append(\"<h3 style='margin-top: 20px;'>トップ10ピークによる主成分分析(PCA)とAIによる軸解釈</h3>\")\n",
    "    try:\n",
    "        top_10_peak_names = final_results.head(10)['ピーク名'].tolist()\n",
    "        X_top10 = features[top_10_peak_names]\n",
    "        pca_top10 = PCA(n_components=2)\n",
    "        pca_top10.fit(X_top10) \n",
    "        df_pca_loadings = pd.DataFrame(pca_top10.components_, columns=top_10_peak_names, index=['PC1', 'PC2'])\n",
    "        explained_variance = pca_top10.explained_variance_ratio_\n",
    "        report_parts.append(f\"<p>サンプル間の違いに最も寄与した重要ピーク（トップ10）を用いて主成分分析(PCA)を行ったところ、第1主成分(PC1)と第2主成分(PC2)で全体の <b>{sum(explained_variance):.1%}</b> の情報を説明できました（PC1: {explained_variance[0]:.1%}, PC2: {explained_variance[1]:.1%}）。</p>\")\n",
    "        \n",
    "        # PC1\n",
    "        pc1_loadings = df_pca_loadings.loc['PC1'].sort_values()\n",
    "        pc1_pos_peaks = pc1_loadings.tail(3).iloc[::-1]\n",
    "        pc1_neg_peaks = pc1_loadings.head(3)\n",
    "        report_parts.append(\"<p><b>第1主成分 (PC1)</b> は、主に以下のピークの増減によって特徴づけられます：</p><ul>\")\n",
    "        report_parts.append(\"<li><b>[+] 正の寄与：</b> \" + \"、\".join([f\"{format_peak_with_chemical(p, peak_annotation_dict)} ({v:.2f})\" for p, v in pc1_pos_peaks.items()]) + \"</li>\")\n",
    "        report_parts.append(\"<li><b>[-] 負の寄与：</b> \" + \"、\".join([f\"{format_peak_with_chemical(p, peak_annotation_dict)} ({v:.2f})\" for p, v in pc1_neg_peaks.items()]) + \"</li>\")\n",
    "        report_parts.append(\"</ul>\")\n",
    "        pc1_pos_aromas = parse_aroma_descriptors(pc1_pos_peaks.index, peak_annotation_dict)\n",
    "        pc1_neg_aromas = parse_aroma_descriptors(pc1_neg_peaks.index, peak_annotation_dict)\n",
    "        report_parts.append(\"<div style='margin-left: 20px; border-left: 3px solid #007bff; padding-left: 15px; background-color: #f4f8ff;'>\")\n",
    "        report_parts.append(\"<h4>[AIによるPC1軸（横軸）の解釈]</h4>\")\n",
    "        top_pos_aroma = pc1_pos_aromas.most_common(1)[0][0] if pc1_pos_aromas else \"不明な香り\"\n",
    "        top_neg_aroma = pc1_neg_aromas.most_common(1)[0][0] if pc1_neg_aromas else \"不明な香り\"\n",
    "        pos_keywords = format_aroma_keywords(pc1_pos_aromas, 3)\n",
    "        neg_keywords = format_aroma_keywords(pc1_neg_aromas, 3)\n",
    "        report_parts.append(f\"<p>PC1（横軸）は、サンプルの香りの質を分ける最も重要な軸であり、「<b>{neg_keywords}</b>」のプロファイルと「<b>{pos_keywords}</b>」のプロファイルとの間の**対比**を示していると解釈できます。</p>\")\n",
    "        report_parts.append(\"<ul>\")\n",
    "        report_parts.append(f\"<li><b>プロットの右側 (PC1が正)</b> にプロットされるサンプルは、{format_peak_with_chemical(pc1_pos_peaks.index[0], peak_annotation_dict, True)} などに由来する「<b>{top_pos_aroma}</b>」様の香りが強いことを示します。</li>\")\n",
    "        report_parts.append(f\"<li><b>プロットの左側 (PC1が負)</b> にプロットされるサンプルは、{format_peak_with_chemical(pc1_neg_peaks.index[0], peak_annotation_dict, True)} などに由来する「<b>{top_neg_aroma}</b>」様の香りが強いことを示します。</li>\")\n",
    "        report_parts.append(\"</ul><p>したがって、この軸は2つの異なる香りのタイプのどちらが優勢であるかを示す「指標」として機能します。</p>\")\n",
    "        report_parts.append(\"</div>\")\n",
    "\n",
    "        # PC2\n",
    "        pc2_loadings = df_pca_loadings.loc['PC2'].sort_values()\n",
    "        pc2_pos_peaks = pc2_loadings.tail(3).iloc[::-1]\n",
    "        pc2_neg_peaks = pc2_loadings.head(3)\n",
    "        report_parts.append(\"<p style='margin-top:15px;'><b>第2主成分 (PC2)</b> は、主に以下のピークの増減によって特徴づけられます：</p><ul>\")\n",
    "        report_parts.append(\"<li><b>[+] 正の寄与：</b> \" + \"、\".join([f\"{format_peak_with_chemical(p, peak_annotation_dict)} ({v:.2f})\" for p, v in pc2_pos_peaks.items()]) + \"</li>\")\n",
    "        report_parts.append(\"<li><b>[-] 負の寄与：</b> \" + \"、\".join([f\"{format_peak_with_chemical(p, peak_annotation_dict)} ({v:.2f})\" for p, v in pc2_neg_peaks.items()]) + \"</li>\")\n",
    "        report_parts.append(\"</ul>\")\n",
    "        pc2_pos_aromas = parse_aroma_descriptors(pc2_pos_peaks.index, peak_annotation_dict)\n",
    "        pc2_neg_aromas = parse_aroma_descriptors(pc2_neg_peaks.index, peak_annotation_dict)\n",
    "        report_parts.append(\"<div style='margin-left: 20px; border-left: 3px solid #28a745; padding-left: 15px; background-color: #f4fff8;'>\")\n",
    "        report_parts.append(\"<h4>[AIによるPC2軸（縦軸）の解釈]</h4>\")\n",
    "        pos_keywords_pc2 = format_aroma_keywords(pc2_pos_aromas, 3)\n",
    "        neg_keywords_pc2 = format_aroma_keywords(pc2_neg_aromas, 3)\n",
    "        report_parts.append(f\"<p>PC2（縦軸）は、PC1（横軸）とは異なる、2番目の香りの対比軸です。この軸は、「<b>{neg_keywords_pc2}</b>」に関連する香りと、「<b>{pos_keywords_pc2}</b>」に関連する香りとの間の対比を示しています。</p>\")\n",
    "        report_parts.append(\"<ul>\")\n",
    "        report_parts.append(f\"<li><b>プロットの上側 (PC2が正)</b> にプロットされるサンプルは、「<b>{pos_keywords_pc2}</b>」に関連する香りが特徴的であることを示します。</li>\")\n",
    "        report_parts.append(f\"<li><b>プロットの下側 (PC2が負)</b> にプロットされるサンプルは、「<b>{neg_keywords_pc2}</b>」に関連する香りが特徴的であることを示します。</li>\")\n",
    "        report_parts.append(\"</ul>\")\n",
    "        report_parts.append(f\"<p>このPC2軸は、PC1軸だけでは説明できない「香りの第二の個性」を表現しています。これにより、グループ間のさらなる詳細な違い（例：同じ「{top_pos_aroma}」様の香りでも、「{pos_keywords_pc2}」のニュアンスが強いか弱いか）を理解するために役立ちます。</p>\")\n",
    "        report_parts.append(\"</div>\")\n",
    "\n",
    "    except Exception as e:\n",
    "        report_parts.append(f\"<p style='color:orange;'>PCA結果の解釈中にエラーが発生しました: {e}</p>\")\n",
    "\n",
    "    \n",
    "    all_group_top_peaks = {}\n",
    "    \n",
    "    # --- (各グループの詳細プロファイル) ---\n",
    "    for group in unique_groups:\n",
    "        report_parts.append(f\"<h2>グループ '{group}' の特徴プロファイル</h2>\")\n",
    "        current_group_summary_points = []\n",
    "        \n",
    "        # 1. 均一性 (変更なし)\n",
    "        dist_rank = sorted(internal_distances, key=internal_distances.get).index(group) + 1\n",
    "        report_parts.append(f\"<h3>1. 均一性（まとまり具合）</h3><p>内部距離は <b>{internal_distances[group]:.2f}</b> であり、{len(unique_groups)}グループ中 <b>{dist_rank}番目</b>に均一な（まとまりが良い）グループです。</p>\")\n",
    "        \n",
    "        # --- (★ ここがバグの修正点 ★) ---\n",
    "        # 2. 他のグループとの関係性\n",
    "        \n",
    "        # (★ 修正: 文字列キー f\"{...}\" ではなく、タプルキー tuple(sorted(...)) を使用 ★)\n",
    "        relations = [(other, inter_group_distances.get(tuple(sorted((group, other))), 0)) for other in unique_groups if group != other]\n",
    "        \n",
    "        if relations:\n",
    "            closest_group = min(relations, key=lambda x: x[1])\n",
    "            farthest_group = max(relations, key=lambda x: x[1])\n",
    "            report_parts.append(f\"<h3>2. 他のグループとの関係性</h3><p>プロファイルが最も近いのは<b>'{closest_group[0]}'</b>（距離: {closest_group[1]:.2f}）、最も遠いのは<b>'{farthest_group[0]}'</b>（距離: {farthest_group[1]:.2f}）です。</p>\")\n",
    "        # --- (★ 修正ここまで ★) ---\n",
    "\n",
    "        # 3. 化学的特徴（Fold Change） (変更なし)\n",
    "        report_parts.append(\"<h3>3. このグループを特徴づける重要ピーク (相対比較)</h3>\")\n",
    "        mean_this_group = df_analysis[df_analysis['group'] == group][features.columns].mean()\n",
    "        mean_other_groups = df_analysis[df_analysis['group'] != group][features.columns].mean()\n",
    "        fold_change = (mean_this_group + 1e-9) / (mean_other_groups + 1e-9)\n",
    "        top_peaks_fc = fold_change.loc[final_results.head(30)['ピーク名']].sort_values(ascending=False)\n",
    "        up_regulated = top_peaks_fc[top_peaks_fc > 1.5].head(5)\n",
    "        down_regulated = top_peaks_fc[top_peaks_fc < 1/1.5].tail(5).sort_values()\n",
    "        if not up_regulated.empty:\n",
    "            report_parts.append(\"<p>他のグループ全体との比較で、このグループで特に<b>量が多い</b>と判断された重要ピーク（トップ5）：</p><ul>\" + \"\".join([f\"<li>{format_peak_with_chemical(peak, peak_annotation_dict)} (約{fc_val:.1f}倍)</li>\" for peak, fc_val in up_regulated.items()]) + \"</ul>\")\n",
    "        if not down_regulated.empty:\n",
    "            report_parts.append(\"<p>他のグループ全体との比較で、このグループで特に<b>量が少ない</b>と判断された重要ピーク（トップ5）：</p><ul>\" + \"\".join([f\"<li>{format_peak_with_chemical(peak, peak_annotation_dict)} (約{fc_val:.2f}倍)</li>\" for peak, fc_val in down_regulated.items()]) + \"</ul>\")\n",
    "        if up_regulated.empty and down_regulated.empty:\n",
    "            report_parts.append(\"<p>他のグループと比較して、突出して多い、または少ないと判断された重要ピーク（トップ30以内）は見つかりませんでした。</p>\")\n",
    "\n",
    "        # 4. モデルによる識別性 (変更なし)\n",
    "        report_parts.append(\"<h3>4. モデルによる識別性（分類性能）</h3>\")\n",
    "        try:\n",
    "            f1_score = report_df.loc[group, 'F1スコア']\n",
    "            unknown_sample_data = df_analysis.loc[df_analysis['group'] == group, features.columns].mean().values\n",
    "            sample_df = pd.DataFrame([unknown_sample_data], columns=features.columns)\n",
    "            prediction = model.predict(sample_df)[0]\n",
    "            probabilities = model.predict_proba(sample_df)\n",
    "            prob_df = pd.DataFrame(probabilities, columns=model.classes_, index=['予測確率']).T\n",
    "            self_prob = prob_df.loc[group, '予測確率']\n",
    "            report_parts.append(f\"<p>モデルはこのグループを <b>{f1_score:.2f} のF1スコア</b>で識別しました。このグループの「平均プロファイル」を予測させた場合、<b>{self_prob:.1%}</b> の確率で正しく <b>'{prediction}'</b> （{ '正解' if prediction == group else '不正解' }）と予測されました。</p>\")\n",
    "        except Exception as e:\n",
    "             report_parts.append(f\"<p>モデル識別性の計算中にエラーが発生しました: {e}</p>\")\n",
    "\n",
    "        # 5. AIの判断拠（SHAP） (変更なし)\n",
    "        report_parts.append(\"<h3>5. AIの判断根拠（SHAPによる解釈）</h3>\")\n",
    "        try:\n",
    "            class_index = list(model.classes_).index(group)\n",
    "            mean_shap_values = np.mean(shap_values[class_index], axis=0)\n",
    "            shap_df = pd.DataFrame({'ピーク名': features.columns, '平均SHAP値': mean_shap_values})\n",
    "            shap_positive = shap_df.sort_values('平均SHAP値', ascending=False).head(3)\n",
    "            shap_negative = shap_df.sort_values('平均SHAP値', ascending=True).head(3)\n",
    "            report_parts.append(\"<p>AIが「このグループである」と判断する際に、最も強く影響を与えたピークは以下の通りです。</p>\")\n",
    "            report_parts.append(\"<p><b>予測を強めた（正に寄与した）ピーク：</b></p><ul>\")\n",
    "            for _, row in shap_positive.iterrows():\n",
    "                report_parts.append(f\"<li>{format_peak_with_chemical(row['ピーク名'], peak_annotation_dict)} (SHAP値: {row['平均SHAP値']:.3f})</li>\")\n",
    "            report_parts.append(\"</ul>\")\n",
    "            report_parts.append(\"<p><b>予測を弱めた（負に寄与した）ピーク：</b></p><ul>\")\n",
    "            for _, row in shap_negative.iterrows():\n",
    "                report_parts.append(f\"<li>{format_peak_with_chemical(row['ピーク名'], peak_annotation_dict)} (SHAP値: {row['平均SHAP値']:.3f})</li>\")\n",
    "            report_parts.append(\"</ul>\")\n",
    "        except Exception as e:\n",
    "            report_parts.append(f\"<p>SHAP値の解釈中にエラーが発生しました: {e}</p>\")\n",
    "        \n",
    "        # 6. 推定される香りのプロファイル (変更なし)\n",
    "        report_parts.append(\"<h3>6. 推定される香りのプロファイル</h3>\")\n",
    "        aroma_counter = Counter()\n",
    "        aroma_peak_list = []\n",
    "        for peak_name, fc_val in up_regulated.items():\n",
    "            candidate = get_best_peak_candidate(peak_name, peak_annotation_dict)\n",
    "            if candidate:\n",
    "                chem_name = candidate.get('名前', '不明')\n",
    "                descriptor_text = candidate.get('官能的記述子', '').strip()\n",
    "                if descriptor_text:\n",
    "                    aroma_peak_list.append(f\"<li>{chem_name} (<b>{descriptor_text}</b>) (約{fc_val:.1f}倍)</li>\")\n",
    "                    descriptors = re.split(r'[;,\\s、。/]+', descriptor_text)\n",
    "                    valid_descriptors = [d.strip() for d in descriptors if d.strip() and d.strip().lower() not in ['none', '']]\n",
    "                    aroma_counter.update(valid_descriptors)\n",
    "        if not aroma_peak_list:\n",
    "            report_parts.append(\"<p>このグループに特徴的（量が多い）なピークから、明確な香り（官能的記述子）を持つ化学物質は見つかりませんでした。</p>\")\n",
    "            current_group_summary_points.append(\"突出した香り成分は検出されませんでした。\")\n",
    "        else:\n",
    "            report_parts.append(\"<p>このグループで特に多かったピークに関連する「官能的記述子」は以下の通りです：</p>\")\n",
    "            report_parts.append(\"<ul>\" + \"\".join(aroma_peak_list) + \"</ul>\")\n",
    "            if aroma_counter:\n",
    "                report_parts.append(\"<p><b>主要な香りキーワード</b>（トップ5）：</p>\")\n",
    "                top_aromas = \"、\".join([f\"<b>{word}</b> ({count}回)\" for word, count in aroma_counter.most_common(5)])\n",
    "                report_parts.append(f\"<p style='font-size:1.1em; margin-left: 20px;'>{top_aromas}</p>\")\n",
    "                summary_text = f\"<b>[AIによる香りプロファイルの推定]</b>: このグループは、\"\n",
    "                top_word = aroma_counter.most_common(1)[0][0]\n",
    "                summary_text += f\"特に「<b>{top_word}</b>」様（よう）の香りを中心とした、\"\n",
    "                if len(aroma_counter) > 2:\n",
    "                    second_word = aroma_counter.most_common(2)[1][0]\n",
    "                    summary_text += f\"「<b>{second_word}</b>」などのニュアンスも感じられる、\"\n",
    "                summary_text += \"複雑なプロファイルを持つと推定されます。\"\n",
    "                report_parts.append(f\"<p>{summary_text}</p>\")\n",
    "                current_group_summary_points.append(f\"「<b>{top_word}</b>」様を中心とした香りのプロファイル。\")\n",
    "            else:\n",
    "                 current_group_summary_points.append(\"特徴的な香り成分はありましたが、キーワード集計はできませんでした。\")\n",
    "\n",
    "        # 7. 主要ピークの平均強度 (v4.4版、変更なし)\n",
    "        report_parts.append(\"<h3>7. 主要ピークの平均強度 (絶対量・香り解析)</h3>\")\n",
    "        report_parts.append(\"<p>セクション3の「相対比較（倍率）」に対し、ここでは「絶対的な強度（量）」に着目します。これにより、グループの香りの本体（キャラクターノート）を形成している成分を特定します。</p>\")\n",
    "        if not up_regulated.empty:\n",
    "            report_parts.append(\"<p><b>特に量が多かったピークの強度と香り：</b></p><ul>\")\n",
    "            for peak_name, _ in up_regulated.items():\n",
    "                this_val = mean_this_group.get(peak_name, 0)\n",
    "                other_val = mean_other_groups.get(peak_name, 0)\n",
    "                candidate = get_best_peak_candidate(peak_name, peak_annotation_dict)\n",
    "                descriptor_text = candidate.get('官能的記述子', '').strip() if candidate else \"\"\n",
    "                aroma_html = f\" (<b>{descriptor_text}</b>)\" if descriptor_text else \" (香り情報なし)\"\n",
    "                report_parts.append(f\"<li>{format_peak_with_chemical(peak_name, peak_annotation_dict)} {aroma_html}: <b>{this_val:,.1f}</b> (他グループ平均: {other_val:,.1f})</li>\")\n",
    "            report_parts.append(\"</ul>\")\n",
    "        if not down_regulated.empty:\n",
    "            report_parts.append(\"<p><b>特に量が少なかったピークの強度と香り：</b></p><ul>\")\n",
    "            for peak_name, _ in down_regulated.items():\n",
    "                this_val = mean_this_group.get(peak_name, 0)\n",
    "                other_val = mean_other_groups.get(peak_name, 0)\n",
    "                candidate = get_best_peak_candidate(peak_name, peak_annotation_dict)\n",
    "                descriptor_text = candidate.get('官能的記述子', '').strip() if candidate else \"\"\n",
    "                aroma_html = f\" (<b>{descriptor_text}</b>)\" if descriptor_text else \" (香り情報なし)\"\n",
    "                report_parts.append(f\"<li>{format_peak_with_chemical(peak_name, peak_annotation_dict)} {aroma_html}: <b>{this_val:,.1f}</b> (他グループ平均: {other_val:,.1f})</li>\")\n",
    "            report_parts.append(\"</ul>\")\n",
    "        \n",
    "        if not up_regulated.empty:\n",
    "            top_intensity_peak = mean_this_group[up_regulated.index].idxmax()\n",
    "            top_intensity_val = mean_this_group[top_intensity_peak]\n",
    "            top_intensity_name_formatted = format_peak_with_chemical(top_intensity_peak, peak_annotation_dict, show_name_only=True)\n",
    "            top_candidate = get_best_peak_candidate(top_intensity_peak, peak_annotation_dict)\n",
    "            top_descriptor = top_candidate.get('官能的記述子', '').strip() if top_candidate else \"\"\n",
    "            aroma_insight = f\"（「<b>{top_descriptor}</b>」様の香り）\" if top_descriptor else \"\"\n",
    "            report_parts.append(f\"<p><b>[AI考察]</b>: このグループの香りの個性は、特に「<b>{top_intensity_name_formatted}</b>」{aroma_insight} の平均強度が <b>{top_intensity_val:,.1f}</b> と突出して高いことに強く影響されています。このピークが、このグループの主要な香りを決定づける「<b>キャラクターノート</b>」である可能性が極めて高いです。</p>\")\n",
    "            current_group_summary_points.append(f\"「{top_intensity_name_formatted}」{aroma_insight} の強度が <b>{top_intensity_val:,.1f}</b> と突出。\")\n",
    "            all_group_top_peaks[group] = (top_intensity_name_formatted, top_intensity_val, aroma_insight)\n",
    "        \n",
    "        elif not down_regulated.empty:\n",
    "            top_low_peak = mean_this_group[down_regulated.index].idxmin()\n",
    "            top_low_name_formatted = format_peak_with_chemical(top_low_peak, peak_annotation_dict, show_name_only=True)\n",
    "            low_candidate = get_best_peak_candidate(top_low_peak, peak_annotation_dict)\n",
    "            low_descriptor = low_candidate.get('官能的記述子', '').strip() if low_candidate else \"\"\n",
    "            aroma_insight_low = f\"（「<b>{low_descriptor}</b>」様の香り）\" if low_descriptor else \"\"\n",
    "            report_parts.append(f\"<p><b>[AI考察]</b>: このグループは、特徴的に増加しているピークは少ないものの、「<b>{top_low_name_formatted}</b>」{aroma_insight_low} が顕著に少ないことが、他のグループとの違いを生み出している主要因と考えられます。</p>\")\n",
    "            current_group_summary_points.append(f\"「{top_low_name_formatted}」{aroma_insight_low} が顕著に少ないことが特徴。\")\n",
    "        else:\n",
    "            report_parts.append(\"<p><b>[AI考察]</b>: このグループは、突出した強度の増減を示す重要ピークは見られませんでした。平均的なプロファイルを持つ集団である可能性があります。</p>\")\n",
    "            current_group_summary_points.append(\"平均的なプロファイルを持つ。\")\n",
    "        \n",
    "        group_profile_summaries[group] = current_group_summary_points\n",
    "\n",
    "\n",
    "    # --- (★ 最終プロファイリングセクション (v4.2のテーブル版) ★) ---\n",
    "    report_parts.append(\"<h2>全体比較と最終プロファイリング</h2>\")\n",
    "    report_parts.append(\"<p>AIによる各グループのプロファイリング結果のサマリーです。</p>\")\n",
    "    \n",
    "    summary_data_list = []\n",
    "    for group in unique_groups:\n",
    "        # (★ 修正: ここもタプルキー `tuple(sorted(...))` を使用 ★)\n",
    "        relations = [(other, inter_group_distances.get(tuple(sorted((group, other))), 0)) for other in unique_groups if group != other]\n",
    "        closest_group_name = min(relations, key=lambda x: x[1])[0] if relations else \"N/A\"\n",
    "        farthest_group_name = max(relations, key=lambda x: x[1])[0] if relations else \"N/A\"\n",
    "        summary_list_html = \"<ul style='margin: 0; padding-left: 20px; text-align: left;'>\" + \"\".join([f\"<li>{item}</li>\" for item in group_profile_summaries.get(group, [\"N/A\"])]) + \"</ul>\"\n",
    "        summary_data_list.append({\n",
    "            'グループ名': f\"<b>{group}</b>\",\n",
    "            'プロファイル サマリー': summary_list_html,\n",
    "            '最も近いグループ': closest_group_name,\n",
    "            '最も遠いグループ': farthest_group_name\n",
    "        })\n",
    "    summary_df = pd.DataFrame(summary_data_list)\n",
    "    summary_table_html = summary_df.to_html(classes='table comparison-table', escape=False, index=False, border=0, justify='left')\n",
    "    report_parts.append(f\"\"\"\n",
    "    <style>\n",
    "        .summary-table-wrapper .comparison-table th:nth-child(1) {{ width: 20%; }}\n",
    "        .summary-table-wrapper .comparison-table th:nth-child(2) {{ width: 50%; }}\n",
    "        .summary-table-wrapper .comparison-table th:nth-child(3) {{ width: 15%; }}\n",
    "        .summary-table-wrapper .comparison-table th:nth-child(4) {{ width: 15%; }}\n",
    "    </style>\n",
    "    <div class=\"table-wrapper summary-table-wrapper\">\n",
    "        {summary_table_html}\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # --- (★ 総合的な結論 (v4.4版、変更なし) ★) ---\n",
    "    report_parts.append(\"<h3>総合結論</h3>\")\n",
    "    report_parts.append(f\"<p>本分析により、{len(unique_groups)}個のグループは化学的プロファイル、特に主要な香り成分の強度と構成によって明確に分類可能であることが示されました。</p>\")\n",
    "    \n",
    "    # 結論1: サマリーの要約\n",
    "    report_parts.append(f\"<p><b>1. グループの個性:</b> 各グループは、固有の「香りのシグネチャ」を持つことが明らかになりました。\")\n",
    "    try:\n",
    "        if all_group_top_peaks: \n",
    "            top_group_by_intensity = max(all_group_top_peaks, key=lambda g: all_group_top_peaks[g][1])\n",
    "            top_peak_name, top_val, top_aroma = all_group_top_peaks[top_group_by_intensity]\n",
    "            report_parts.append(f\"特に「<b>{top_group_by_intensity}</b>」は、「<b>{top_peak_name}</b>」{top_aroma} の平均強度が <b>{top_val:,.1f}</b> と極めて高く、これがこのグループの個性を決定づける最強の要因となっています。\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    report_parts.append(f\"一方で、「<b>{min_internal_dist_group}</b>」は最も均一な（＝個体差の少ない）安定したプロファイルを持つグループとして同定されました。</p>\")\n",
    "\n",
    "    # 結論2: 相関の意義\n",
    "    report_parts.append(f\"<p><b>2. 香りの相関関係:</b> 「重要香気成分の分析」で示されたように、特定の香気成分\")\n",
    "    if not corr_df.empty: \n",
    "         report_parts.append(f\"（例: {corr_df.iloc[0]['香りのペア (A)']} と {corr_df.iloc[0]['香りのペア (B)']} のペア）\")\n",
    "    report_parts.append(\"が強い相関を持って検出されました。これは、これらの成分が共通の生成経路を持つか、あるいは組み合わさることで特定の「香り」を形成していることを強く示唆しています。サンプル間の違いは、単一の成分ではなく、これらの「香りの組み合わせ（シグネチャ）」によって生まれていると考えられます。</p>\")\n",
    "\n",
    "    # 結論3: PCA軸の意義\n",
    "    report_parts.append(f\"<p><b>3. 分類軸の解釈:</b> PCA分析（トップ10ピーク）は、サンプル間の違いを2つの主要な「対比軸」で説明できることを示しました。\")\n",
    "    report_parts.append(f\"PC1（横軸）は「<b>{format_aroma_keywords(pc1_neg_aromas, 2)}</b>」様の香りと「<b>{format_aroma_keywords(pc1_pos_aromas, 2)}</b>」様の香りのどちらが優勢かを示す軸でした。\")\n",
    "    report_parts.append(f\"PC2（縦軸）は、それに直交する別の香りの質（例: 「<b>{format_aroma_keywords(pc2_pos_aromas, 2)}</b>」様のニュアンス）の強弱を示す軸でした。\")\n",
    "    report_parts.append(f\"全体構造プロット（セル7）で観測されたグループ間の位置関係は、これら2つの「香りの対比軸」によって化学的に説明が可能です。</p>\")\n",
    "    \n",
    "    # 結論4: モデルの妥当性\n",
    "    report_parts.append(f\"<p><b>4. AIモデルの妥当性:</b> 構築されたAIモデル（RandomForest）は、これらの化学的プロファイルの違いを学習し、高い精度（F1スコア）でグループを識別できました。SHAP分析（セクション5）は、AIが「予測を強めたピーク」として挙げたものが、実際に各グループで「量が多いピーク」（セクション3）や「絶対強度が高いピーク」（セクション7）と一致していることを示しており、モデルがデータの化学的特徴を妥当に学習していることを裏付けています。</p>\")\n",
    "    \n",
    "\n",
    "    return \"\".join(report_parts)\n",
    "\n",
    "# --- (関数の実行: CSSスタイルの部分を修正) ---\n",
    "try:\n",
    "    # (★ CSSスタイルを関数呼び出しの「前」に移動 ★)\n",
    "    display(HTML(\"\"\"\n",
    "    <style>\n",
    "        .table { width: 100%; border-collapse: collapse; margin-top: 15px; font-size: 0.9em; }\n",
    "        .table th, .table td { padding: 8px 12px; border: 1px solid #ddd; text-align: left; }\n",
    "        .table th { background-color: #f4f4f4; font-weight: bold; }\n",
    "        .table-striped tbody tr:nth-of-type(odd) { background-color: #f9f9f9; }\n",
    "        .table-striped tbody tr:hover { background-color: #f1f1f1; }\n",
    "        /* ▼▼▼ テーブルラッパーとテキスト整列のスタイル ▼▼▼ */\n",
    "        .table-wrapper {\n",
    "            overflow-x: auto;      /* 横に長いテーブルでスクロールを可能にする */\n",
    "            margin: 20px 0;\n",
    "            border: 1px solid #ddd;\n",
    "        }\n",
    "        .comparison-table td {\n",
    "            word-wrap: break-word;   /* 長い単語を強制的に改行する */\n",
    "            white-space: normal;     /* テキストの自動改行を許可する */\n",
    "            vertical-align: top;     /* セル内のテキストを上揃えにする */\n",
    "            text-align: left;        /* テキストを左揃えにする */\n",
    "        }\n",
    "        .comparison-table th {\n",
    "            text-align: left;        /* ヘッダーも左揃えにする */\n",
    "        }\n",
    "        /* ▲▲▲ 追加ここまで ▲▲▲ */\n",
    "    </style>\n",
    "    \"\"\"))\n",
    "\n",
    "    if 'peak_annotation_dict' not in locals():\n",
    "        print(\"   > 警告: 化学情報辞書(peak_annotation_dict) が見つかりません。\")\n",
    "        print(\"   >      セル3 が正常に実行されているか確認してください。\")\n",
    "        print(\"   >      化学情報なしで考察を生成します。\")\n",
    "        peak_annotation_dict = {} \n",
    "\n",
    "    ai_interpretation_text = generate_ai_interpretation(\n",
    "        groups, \n",
    "        internal_distances, \n",
    "        inter_group_distances, \n",
    "        final_results, \n",
    "        df_analysis, \n",
    "        ordered_groups_for_plotting,\n",
    "        model,\n",
    "        report_df,\n",
    "        shap_values,\n",
    "        features,\n",
    "        peak_annotation_dict\n",
    "    )\n",
    "except NameError as e:\n",
    "    print(f\"❌ AI考察の生成に必要な変数が不足しています: {e}\")\n",
    "    print(\"   > (★) peak_annotation_dict がセル3で正しく定義されているか確認してください。\")\n",
    "    ai_interpretation_text = \"<p style='color:red;'>AI考察の生成に必要なデータが不足していたため、考察を生成できませんでした。</p>\"\n",
    "except Exception as e:\n",
    "    print(f\"❌ AI考察の生成中に予期せぬエラーが発生しました: {e}\")\n",
    "    ai_interpretation_text = f\"<p style='color:red;'>AI考察の生成中にエラーが発生しました: {e}</p>\"\n",
    "\n",
    "\n",
    "print(\"✅ AIによる統合考察（v4.5 グループ間距離バグ修正版）が完了しました。\")\n",
    "display(HTML(ai_interpretation_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# セル27: HTMLレポートに必要な全てのグラフと計算の実行\n",
    "# ===============================================================================\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import japanize_matplotlib\n",
    "from matplotlib.ticker import ScalarFormatter # (★ Y軸の指数表記を無効化するために追加 ★)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import chi2, ttest_ind\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n--- 最終レポート: 機能追加・超詳細解説付きHTMLレポートの生成 ---\")\n",
    "\n",
    "def image_to_base64_html(img_path, max_width=\"80%\"):\n",
    "    if not img_path or not os.path.exists(img_path): return f\"<p style='color: red;'>画像ファイルが見つかりません: {os.path.basename(img_path if img_path else 'N/A')}</p>\"\n",
    "    try:\n",
    "        with open(img_path, \"rb\") as image_file: encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        return f'<img src=\"data:image/png;base64,{encoded_string}\" alt=\"{os.path.basename(img_path)}\" style=\"max-width:{max_width}; height:auto; border:1px solid #ccc; margin-top:10px;\">'\n",
    "    except Exception as e: return f\"<p style='color: red;'>画像読み込みエラー: {e}</p>\"\n",
    "\n",
    "def generate_ai_interpretation():\n",
    "    try:\n",
    "        report_parts, unique_groups = [], ordered_groups_for_plotting\n",
    "        report_parts.append(\"<h2>総合サマリー (簡易版)</h2>\")\n",
    "        min_internal_dist_group, max_internal_dist_group = min(internal_distances, key=internal_distances.get), max(internal_distances, key=internal_distances.get)\n",
    "        if inter_group_distances:\n",
    "            max_inter_dist_pair_key = max(inter_group_distances, key=inter_group_distances.get)\n",
    "            max_inter_dist_pair_str = max_inter_dist_pair_key\n",
    "            report_parts.append(f\"<p>本分析では、{len(unique_groups)}個のグループが特定されました。中でも、グループ<b>'{min_internal_dist_group}'</b>は最も均一性が高く、グループ<b>'{max_internal_dist_group}'</b>は最も多様性に富んだ集団です。また、グループ間の特徴が最も異なっていたのは<b>'{max_inter_dist_pair_str}'</b>のペアでした。</p>\")\n",
    "        else:\n",
    "            report_parts.append(f\"<p>本分析では、{len(unique_groups)}個のグループが特定されました。</p>\")\n",
    "        for group in unique_groups:\n",
    "            report_parts.append(f\"<h3>グループ '{group}' の特徴プロファイル</h3>\")\n",
    "            mean_this_group, mean_other_groups = df_analysis[df_analysis['group'] == group][features.columns].mean(), df_analysis[df_analysis['group'] != group][features.columns].mean()\n",
    "            fold_change = (mean_this_group + 1e-9) / (mean_other_groups + 1e-9)\n",
    "            top_peaks_fc = fold_change[final_results.head(30)['ピーク名']].sort_values(ascending=False)\n",
    "            up_regulated = top_peaks_fc[top_peaks_fc > 1.5].head(5)\n",
    "            if not up_regulated.empty: report_parts.append(\"<p>このグループで特に<b>量が多い</b>と判断された重要ピーク（トップ5）：</p><ul>\" + \"\".join([f\"<li>{peak} (約{fc_val:.1f}倍)</li>\" for peak, fc_val in up_regulated.items()]) + \"</ul>\")\n",
    "            else: report_parts.append(\"<p>突出して多いと判断された重要ピークは見つかりませんでした。</p>\")\n",
    "        return \"\".join(report_parts)\n",
    "    except NameError:\n",
    "        return \"<p style='color:red;'>AI考察の生成に必要な変数(internal_distancesなど)が見つかりませんでした。</p>\"\n",
    "\n",
    "def add_group_ellipses(fig, df, x_col, y_col, group_col):\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    unique_groups = df[group_col].unique().tolist()\n",
    "    color_map = {group: colors[i % len(colors)] for i, group in enumerate(unique_groups)}\n",
    "    for group_name in unique_groups:\n",
    "        subset = df[df[group_col] == group_name]\n",
    "        if len(subset) > 2:\n",
    "            try:\n",
    "                cov = np.cov(subset[x_col], subset[y_col])\n",
    "                eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "                angle = np.degrees(np.arctan2(*eigenvecs[:, 0][::-1]))\n",
    "                scale = np.sqrt(chi2.ppf(0.95, 2))\n",
    "                width, height = 2 * scale * np.sqrt(eigenvals[0]), 2 * scale * np.sqrt(eigenvals[1])\n",
    "                center_x, center_y = subset[x_col].mean(), subset[y_col].mean()\n",
    "                fig.add_shape(type=\"ellipse\", xref=\"x\", yref=\"y\", x0=center_x - width/2, y0=center_y - height/2, x1=center_x + width/2, y1=center_y + height/2,\n",
    "                              line=dict(color=color_map[group_name], width=2), fillcolor=color_map[group_name], opacity=0.15, layer=\"below\",\n",
    "                              xanchor=center_x, yanchor=center_y, rotation=angle)\n",
    "            except Exception as e: pass \n",
    "    return fig\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"   > 警告: 出力ディレクトリ '{output_dir}' が見つかりません。新規に作成します。\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"   > STEP 1/3: 追加分析を実行中...\")\n",
    "\n",
    "    # --- クロマトグラムの描画 ---\n",
    "    plt.rcdefaults()\n",
    "    japanize_matplotlib.japanize()\n",
    "\n",
    "    colors_mpl = plt.cm.viridis(np.linspace(0, 1, len(ordered_groups_for_plotting)))\n",
    "    group_color_map = {group: colors_mpl[i] for i, group in enumerate(ordered_groups_for_plotting)}\n",
    "    \n",
    "    # ★★★★★★★★★★★★★★★★★★★★★★★ 修正箇所 ★★★★★★★★★★★★★★★★★★★★★★★\n",
    "    # 正規表現(re.match)が環境によってうまく機能しない問題に対処するため、\n",
    "    # より確実な文字列操作による判定方法に変更します。\n",
    "    def is_peak_column(col_name):\n",
    "        \"\"\"カラム名が '数字-数字' の形式か判定する関数\"\"\"\n",
    "        s = str(col_name)\n",
    "        if s.count('-') == 1:\n",
    "            part1, part2 = s.split('-')\n",
    "            return part1.isdigit() and part2.isdigit()\n",
    "        return False\n",
    "\n",
    "    all_peak_columns = [col for col in df_analysis.columns if is_peak_column(col)]\n",
    "    # ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "\n",
    "    cols_1 = ['group'] + [col for col in all_peak_columns if col.endswith('-1')]\n",
    "    df_col1 = df_analysis.reindex(columns=cols_1, fill_value=0) # reindexで安全に列を選択\n",
    "    cols_2 = ['group'] + [col for col in all_peak_columns if col.endswith('-2')]\n",
    "    df_col2 = df_analysis.reindex(columns=cols_2, fill_value=0) # reindexで安全に列を選択\n",
    "    \n",
    "    group_chromatogram_paths_col1, group_chromatogram_paths_col2 = [], []\n",
    "    all_groups_chromatogram_path_col1, all_groups_chromatogram_path_col2 = None, None\n",
    "\n",
    "    column_info = [\n",
    "        (df_col1, \"MXT-5-FID1 (カラム1相当)\", \"1\", group_chromatogram_paths_col1),\n",
    "        (df_col2, \"MXT-WAX-FID2 (カラム2相当)\", \"2\", group_chromatogram_paths_col2)\n",
    "    ]\n",
    "\n",
    "    for df, col_name, suffix, path_list in column_info:\n",
    "        # 'group'列以外のピーク列があるかチェック\n",
    "        if df.shape[1] <= 1: \n",
    "            print(f\"     > カラム {col_name} のデータが見つからないため、クロマトグラム描画をスキップします。\")\n",
    "            continue\n",
    "\n",
    "        # --- 全グループの平均プロファイル比較 ---\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        mean_profile_df = df.drop(columns=['group']).groupby(df['group']).mean().T\n",
    "        peak_labels = [str(lbl) for lbl in mean_profile_df.index.tolist()]\n",
    "        peak_indices = np.arange(len(peak_labels))\n",
    "        \n",
    "        for i, group_name in enumerate(ordered_groups_for_plotting):\n",
    "            if group_name in mean_profile_df.columns:\n",
    "                color = group_color_map.get(group_name)\n",
    "                plt.plot(peak_indices, mean_profile_df[group_name].values, label=group_name, alpha=0.8, color=color, linewidth=2)\n",
    "\n",
    "        plt.title(f'全グループの平均プロファイル比較 ({col_name})', fontsize=24, pad=20)\n",
    "        plt.xlabel('ピークID（ピークの並び順）', fontsize=16)\n",
    "        plt.ylabel('平均強度（面積）', fontsize=16)\n",
    "        step = max(1, len(peak_indices)//20)\n",
    "        plt.xticks(peak_indices[::step], peak_labels[::step], rotation=90, fontsize=8)\n",
    "        plt.legend(title='グループ', fontsize=12, title_fontsize=14)\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "        plt.gca().yaxis.set_major_formatter(ScalarFormatter(useMathText=False))\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filepath_all = os.path.join(output_dir, f'全グループ比較クロマトグラム_カラム{suffix}.png')\n",
    "        plt.savefig(filepath_all, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        if suffix == \"1\": all_groups_chromatogram_path_col1 = filepath_all\n",
    "        else: all_groups_chromatogram_path_col2 = filepath_all\n",
    "        print(f\"     ✅ [復元] {col_name} の全グループ比較クロマトグラムを保存しました。\")\n",
    "        \n",
    "        # --- 各グループ内の全サンプルプロファイル ---\n",
    "        for i, group_name in enumerate(ordered_groups_for_plotting):\n",
    "            group_data = df[df['group'] == group_name].drop(columns=['group'])\n",
    "            if group_data.empty: continue\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            plt.plot(group_data.T.values, color='grey', alpha=0.2)\n",
    "            mean_profile = group_data.mean()\n",
    "            \n",
    "            color = group_color_map.get(group_name)\n",
    "            plt.plot(mean_profile.values, color=color, linewidth=3, label=f'{group_name} 平均')\n",
    "\n",
    "            plt.title(f'グループ「{group_name}」内の全サンプルプロファイル ({col_name})', fontsize=16, pad=20)\n",
    "            plt.xlabel('ピークID（ピークの並び順）', fontsize=12)\n",
    "            plt.ylabel('強度（面積）', fontsize=12)\n",
    "            \n",
    "            peak_labels_group = [str(lbl) for lbl in group_data.columns.tolist()]\n",
    "            peak_indices_group = np.arange(len(peak_labels_group))\n",
    "            step_local = max(1, len(peak_indices_group)//15)\n",
    "            plt.xticks(peak_indices_group[::step_local], peak_labels_group[::step_local], rotation=90, fontsize=8)\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "            plt.gca().yaxis.set_major_formatter(ScalarFormatter(useMathText=False))\n",
    "            plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            filepath_group = os.path.join(output_dir, f'クロマトグラム_カラム{suffix}_{group_name}.png')\n",
    "            plt.savefig(filepath_group, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            path_list.append(filepath_group)\n",
    "        print(f\"     ✅ [復元] {col_name} の各グループのクロマトグラムを個別に保存しました。\")\n",
    "\n",
    "    # (ダミーモデルの作成)\n",
    "    if 'model' not in locals():\n",
    "        print(\"     > [警告] `model`変数が未定義です。ダミーのRandomForestモデルを作成して続行します。\")\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(StandardScaler().fit_transform(features), groups)\n",
    "        if 'cv_accuracy' not in locals(): cv_accuracy = 0.95\n",
    "        if 'report_df' not in locals(): report_df = pd.DataFrame(classification_report(groups, model.predict(StandardScaler().fit_transform(features)), output_dict=True)).transpose()\n",
    "\n",
    "    print(\"     > [復元] モデル性能評価、距離分析、補足分析のグラフを生成中...\")\n",
    "    \n",
    "    print(\"   > STEP 1/3: 追加分析が完了しました。\")\n",
    "\n",
    "except Exception as e:\n",
    "    plt.rcdefaults()\n",
    "    print(f\"   > ❌ ERROR: 追加分析の実行中にエラーが発生しました: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    group_chromatogram_paths_col1, group_chromatogram_paths_col2 = [], []\n",
    "    all_groups_chromatogram_path_col1, all_groups_chromatogram_path_col2 = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a504b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# セル28: 最終HTMLレポートの生成\n",
    "# =============================================================================\n",
    "try:\n",
    "    # ★★★ 追加: Python環境のバージョン情報を取得 ★★★\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    import plotly\n",
    "    import shap\n",
    "    \n",
    "    python_version = sys.version.split(' ')[0]\n",
    "    pandas_version = pd.__version__\n",
    "    sklearn_version = sklearn.__version__\n",
    "    plotly_version = plotly.__version__\n",
    "    shap_version = shap.__version__\n",
    "    \n",
    "    env_info_html = f\"\"\"\n",
    "    <div class=\"interpretation\" style=\"margin-top: 40px; font-size: 12px; color: #666;\">\n",
    "        <p style=\"margin:0;\"><b>解析環境情報:</b> \n",
    "        Python {python_version} | \n",
    "        pandas {pandas_version} | \n",
    "        scikit-learn {sklearn_version} | \n",
    "        plotly {plotly_version} |\n",
    "        shap {shap_version}\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"   > STEP 2/3: HTMLレポートを生成中...\")\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html><html lang=\"ja\"><head><meta charset=\"UTF-8\"><title>Heracles NEO 総合解析レポート</title>\n",
    "    <style>\n",
    "        body { \n",
    "            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", \"Hiragino Sans\", \"Hiragino Kaku Gothic ProN\", Meiryo, sans-serif;\n",
    "            b, strong { font-weight: bold; }\n",
    "            line-height: 1.7; \n",
    "            margin: 0 auto; \n",
    "            max-width: 950px; \n",
    "            padding: 20px; \n",
    "            color: #333; \n",
    "        }\n",
    "        h1 { font-size: 26px; border-bottom: 3px solid #4A90E2; padding-bottom: 10px; margin-top: 50px; }\n",
    "        h2 { font-size: 20px; border-left: 6px solid #4A90E2; padding-left: 10px; margin-top: 40px; background-color: #f2f8ff; }\n",
    "        h3 { font-size: 16px; margin-top: 30px; border-bottom: 1px dashed #4A90E2; padding-bottom: 5px; color: #4A90E2; }\n",
    "        h4 { font-size: 15px; margin-top: 25px; margin-bottom: 10px; border-bottom: 1px solid #eee; padding-bottom: 5px;}\n",
    "        p, li { font-size: 14px; }\n",
    "        ul { padding-left: 20px; }\n",
    "        \n",
    "        .table-wrapper {\n",
    "            overflow-x: auto;\n",
    "            -webkit-overflow-scrolling: touch;\n",
    "            margin: 20px 0;\n",
    "            box-shadow: 0 2px 3px rgba(0,0,0,0.1);\n",
    "            border: 1px solid #ddd;\n",
    "        }\n",
    "        table { \n",
    "            border-collapse: collapse; \n",
    "            width: 100%; \n",
    "            margin: 0;\n",
    "        }\n",
    "        th, td { \n",
    "            border: 1px solid #ddd; \n",
    "            padding: 8px 12px; \n",
    "            text-align: left; \n",
    "            white-space: nowrap; \n",
    "            font-size: 13px; \n",
    "        }\n",
    "        th { background-color: #f2f8ff; font-weight: bold; }\n",
    "        .comparison-table tr:nth-child(even) { background-color: #f9f9f9; }\n",
    "        \n",
    "        .center { text-align: center; }\n",
    "        .interpretation { background-color: #f9f9f9; border: 1px solid #eee; border-left: 5px solid #6c757d; padding: 15px; margin: 20px 0; border-radius: 5px; }\n",
    "        .summary { background-color: #e6f7ff; border: 1px solid #91d5ff; padding: 20px; margin-top: 20px; border-radius: 8px; }\n",
    "        \n",
    "        .flex-container { \n",
    "            display: flex; \n",
    "            flex-wrap: wrap; \n",
    "            justify-content: center; \n",
    "            align-items: flex-start; \n",
    "            width: 100%; \n",
    "        }\n",
    "        .flex-item { \n",
    "            flex-grow: 0; \n",
    "            flex-shrink: 1;\n",
    "            flex-basis: 45%; \n",
    "            min-width: 300px; \n",
    "            margin: 10px; \n",
    "        }\n",
    "        .formula { text-align: center; font-size: 1.2em; margin: 20px; padding: 10px; background-color: #fff; border: 1px solid #ddd; border-radius: 5px; }\n",
    "        .highlight-box { background-color: #fffbe6; border: 1px solid #ffe58f; padding: 10px; border-radius: 4px; margin-top: 10px; }\n",
    "    </style>\n",
    "    <script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js\"></script>\n",
    "    </head><body>\n",
    "    \"\"\"\n",
    "    html_content += f\"<h1 class='center' style='border:none;'>Heracles NEO 総合解析レポート</h1><p class='center'>作成日時: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\"\n",
    "    html_content += \"<h1>1. はじめに</h1>\"\n",
    "    html_content += \"<h2>1.1. 解析の目的</h2><div class='interpretation'><p>本レポートは、提供されたサンプルデータ（各サンプルから検出された化学物質ピークの強度データ）に基づき、事前に定義された各グループの化学的プロファイルにどのような違いがあるかを多角的に解析した結果をまとめたものです。<br>主な目的は、以下の通りです。</p><ul><li>グループ間の違いを統計学的な観点から客観的に裏付けること。</li><li>グループの違いに最も強く貢献している主要な要因（化学物質ピーク）を特定すること。</li><li>各グループの化学的な「個性」や「特徴」を明確に言語化・可視化すること。</li></ul><p>これにより、感覚的な評価だけでなく、データに基づいた科学的な議論や意思決定を支援します。</p></div>\"\n",
    "    html_content += \"<h2>1.2. データ概要</h2>\"\n",
    "    if 'df_analysis' in locals():\n",
    "        group_counts_str = \"\".join([f\"<li><b>{grp}:</b> {count}件</li>\" for grp, count in groups.value_counts().items()])\n",
    "        html_content += f\"<ul><li><b>分析対象サンプル数:</b> {len(df_analysis.index)}件</li><li><b>ピーク（特徴量）数:</b> {len(features.columns)}種類</li><li><b>サンプル単位での外れ値除去数 (PCA):</b> {len(set(rejected_samples))}件</li><li><b>ピーク単位での外れ値棄却数 (Z-score):</b> {len(outlier_peaks_info)}点</li><li><b>検出されたグループ:</b><ul>{group_counts_str}</ul></li></ul>\"\n",
    "    \n",
    "    html_content += \"<h1>2. 解析結果と考察</h1>\"\n",
    "    \n",
    "    html_content += \"<h2>2.1. プロファイルプロット（クロマトグラム）による全体像の把握</h2>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の目的</h3><p>全てのピーク（化学物質）の強度を、検出された順番に沿って線で結ぶことで、各サンプルやグループの全体的な化学的プロファイル（<b>化学的指紋</b>のようなもの）を視覚的に把握します。数百〜数千の数値の羅列を直感的な「パターン」として捉えることで、グループ内のばらつきや、グループ間のパターン形状の違いを一目で確認することができます。</p><h3>解析の見方</h3><p>グラフのX軸はピークの出現順、Y軸はその強度（量）を示します。「山の形状」や「谷の位置」、「どのピークが特に高いか」といった全体的なパターンに注目してください。<b>グループ間で特定の山の高さが大きく異なる場合、その山に対応する化学物質がグループ間の違いを生み出す重要な要因である</b>可能性を示唆しています。<br>本分析では、2本の異なる特性を持つカラムで測定されたため、それぞれのカラムに対応する2種類のプロットが表示されます。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h3>カラム1: MXT-5-FID1</h3>\"\n",
    "    html_content += f\"<div class='center'>{image_to_base64_html(os.path.join(output_dir, '全グループ比較クロマトグラム_カラム1.png'), max_width='95%')}</div>\"\n",
    "    html_content += \"<h4>グループ内プロファイル (MXT-5-FID1)</h4><div class='flex-container'>\"\n",
    "    for path in group_chromatogram_paths_col1:\n",
    "        html_content += f\"<div class='flex-item'>{image_to_base64_html(path, max_width='100%')}</div>\"\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    html_content += \"<h3>カラム2: MXT-WAX-FID2</h3>\"\n",
    "    html_content += f\"<div class='center'>{image_to_base64_html(os.path.join(output_dir, '全グループ比較クロマトグラム_カラム2.png'), max_width='95%')}</div>\"\n",
    "    html_content += \"<h4>グループ内プロファイル (MXT-WAX-FID2)</h4><div class='flex-container'>\"\n",
    "    for path in group_chromatogram_paths_col2:\n",
    "        html_content += f\"<div class='flex-item'>{image_to_base64_html(path, max_width='100%')}</div>\"\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>各カラムのプロットからは、グループ内である程度一貫したパターンが確認できます。全グループの平均プロファイルを一枚のグラフに重ねて比較すると、カラムごとに異なるピーク領域でグループ間に明確な強度の差が見られます。これらのパターン差が、後続の多変量解析で観測されるグループ分離の根源となっており、グループが化学的に異なる特徴を持つことの最初の視覚的な証拠となります。</p></div>\"\n",
    "\n",
    "    html_content += \"<h2>2.2. 全体構造の可視化と主要ピークの特定</h2>\"\n",
    "    html_content += \"\"\"\n",
    "    <div class='interpretation'>\n",
    "        <h3>解析の目的</h3>\n",
    "        <p>数百〜数千種類にも及ぶピーク（特徴量）によって構成される「多次元空間」のデータを、人間が理解できる2次元の散布図に投影（次元削減）します。これは、<b>複雑な地形を持つ地球全体を、一枚の世界地図に描き出す作業</b>に似ています。これにより、サンプル全体の構造、グループ間の関係性（どのグループが似ていて、どのグループが離れているか）、およびデータセット内のばらつきの傾向を視覚的に把握します。異なるアルゴリズム（地図の描き方）を用いることで、データの異なる側面を照らし出します。</p>\n",
    "        <h3>解析のメカニズム</h3>\n",
    "        <p>それぞれ得意なこと（地図の描き方のルール）が異なる複数の次元削減手法を用いて可視化を行います。</p>\n",
    "        <div class=\"table-wrapper\"><table class=\"table comparison-table\">\n",
    "            <thead><tr><th>手法</th><th>グループ情報の利用</th><th>アルゴリズムの概要（一言で言うと？）</th></tr></thead>\n",
    "            <tbody>\n",
    "                <tr><td><b>PCA (主成分分析)</b></td><td>利用しない (教師なし)</td><td>「データのばらつきが最も大きい方向」を探す、最も基本的な\"地図\"の描き方。全体の傾向を掴むのに適しています。</td></tr>\n",
    "                <tr><td><b>PLS-DA (部分的最小二乗判別分析)</b></td><td>利用する (教師あり)</td><td>グループの違いを強調するように地図を描くため、グループ間の差が分かりやすくなります。<b>どのピークがその差に貢献しているか</b>も同時に分析します。</td></tr>\n",
    "                <tr><td><b>LDA (線形判別分析)</b></td><td>利用する (教師あり)</td><td>「同じグループは近くに、違うグループは遠くに」配置することを最優先する描き方。グループを分離する力は最も強力です。</td></tr>\n",
    "                <tr><td><b>階層的クラスタリング</b></td><td>利用しない (教師なし)</td><td>サンプル同士の類似度を計算し、似ているものから順に繋げていくことで「家系図」のようなものを作成します。サンプル間の系統関係やグループの成り立ちを探るのに役立ちます。</td></tr>\n",
    "                <tr><td><b>UMAP / t-SNE</b></td><td>利用しない (教師なし)</td><td>「ご近所付き合い」（サンプル間の局所的な距離関係）をなるべく維持したまま2次元に配置する、最新の高度な描き方。複雑なデータの構造を捉えるのが得意です。</td></tr>\n",
    "            </tbody>\n",
    "        </table></div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    html_content += \"<h3>解析の結果の図表</h3>\"\n",
    "    \n",
    "    html_content += \"<h4>Part 1: 全ピークを用いた全体構造の可視化</h4>\"\n",
    "    html_content += \"<div class='interpretation' style='margin-top:10px;'><p>まず、データセットに含まれる全てのピーク情報を用いて、各手法でサンプル全体の構造を可視化します。これにより、データの大局的な特徴、つまり「森」全体を俯瞰します。</p></div>\"\n",
    "    html_content += \"<div class='flex-container'>\"\n",
    "    html_content += f\"<div class='flex-item'>{image_to_base64_html(os.path.join(output_dir, 'PLS-DAプロット.png'), max_width='95%')}</div>\"\n",
    "    html_content += f\"<div class='flex-item'>{image_to_base64_html(os.path.join(output_dir, 'PCAプロット.png'), max_width='95%')}</div>\"\n",
    "    html_content += f\"<div class='flex-item'>{image_to_base64_html(os.path.join(output_dir, 'LDAプロット.png'), max_width='95%')}</div>\"\n",
    "    html_content += f\"<div class='flex-item'>{image_to_base64_html(os.path.join(output_dir, 'UMAPプロット.png'), max_width='95%')}</div>\"\n",
    "    html_content += f\"<div class='flex-item' style='flex-basis: 100%; text-align: center;'>{image_to_base64_html(os.path.join(output_dir, 't-SNEプロット.png'), max_width='48%')}</div>\"\n",
    "    html_content += f\"<div class='flex-item' style='flex-basis: 100%; text-align: center;'>{image_to_base64_html(os.path.join(output_dir, '階層的クラスタリング.png'), max_width='90%')}</div>\"\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    html_content += \"<h4 style='margin-top:40px;'>Part 2: 各手法における主要ピークの特定</h4>\"\n",
    "    html_content += \"\"\"\n",
    "    <div class='interpretation' style='margin-top:10px;'>\n",
    "        <p>上記の散布図において、なぜグループが分かれて見えるのでしょうか？その「なぜ」に答えるため、各手法がグループを分離する上で特に重視したピークを特定します。これにより、地図上で国々が分かれている「国境」の役割を果たしているピークが明らかになります。</p>\n",
    "        <div class='interpretation' style='background-color: #e9ecef;'>\n",
    "            <h5>【グラフと表の見方・解釈の仕方】</h5>\n",
    "            <p>これらの指標を理解することで、グラフの背後にある化学的な意味を読み解くことができます。</p>\n",
    "            <ul>\n",
    "                <li><b>LDA寄与度スコア:</b> LDAはグループを最もよく分ける「境界線（判別軸）」を引く手法です。このスコアが高いピークほど、その境界線を決定づける力が強い、つまり<b>グループを見分けるための「最も重要なものさし」</b>であることを意味します。LDAプロット上でグループが分離しているのは、主にこれらのピークの値が異なるためであると解釈できます。</li>\n",
    "                <li><b>Feature Importance (代理指標):</b> UMAPやt-SNEは複雑な計算を行うため、直接的な寄与度は算出できません。そこで、同様にグループ分類を得意とする機械学習モデル（ランダムフォレスト）を代理として用い、そのモデルが「分類のために重要視したピーク」をリストアップしています。この値が高いピークほど、<b>UMAPやt-SNEのプロット上でサンプルが\"引き寄せ合って\"まとまりを形成する要因となっている可能性が高い</b>と解釈できます。</li>\n",
    "            </ul>\n",
    "            <p class='highlight-box'><b>（具体的な解釈例）</b>もし「ピークX」がLDAの寄与度スコアで1位であったなら、「LDAプロットで観測されるグループ間の分離は、主に\"ピークX\"という一本のものさしで測った値の違いによって生まれている」と考えることができます。</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    html_content += \"<div class='flex-container'>\"\n",
    "    html_content += f\"<div class='flex-item'><h5>LDA 上位10寄与ピーク</h5>{image_to_base64_html(os.path.join(output_dir, 'LDA_Top10_Peaks.png'), max_width='95%')}<div class='table-wrapper'>{lda_top10_html}</div></div>\"\n",
    "    html_content += f\"<div class='flex-item'><h5>UMAP/t-SNE 上位10寄与ピーク</h5>{image_to_base64_html(os.path.join(output_dir, 'RF_Top10_Peaks_Proxy.png'), max_width='95%')}<div class='table-wrapper'>{rf_top10_html_for_umap_tsne}</div></div>\"\n",
    "    html_content += \"</div>\"\n",
    "\n",
    "    html_content += \"<h4 style='margin-top:40px;'>Part 3: 総合ランク上位10ピークによる再解析</h4>\"\n",
    "    html_content += \"<div class='interpretation' style='margin-top:10px;'><p>最後に、後述する統計解析と機械学習を統合して算出した「総合ランク」の上位10ピークのみを用いて、再度PCAとPLS-DAを行います。これは、<b>数ある情報の中から特に重要なものだけを厳選し、それら少数精鋭のピークだけでグループをどれだけ明確に分離できるか、その識別能力を検証する</b>ストレステストのようなものです。ここで綺麗に分離できれば、それらのピークが本質的に重要であることの強力な証拠となります。</p></div>\"\n",
    "    html_content += \"<h5>表: 識別力の高い上位10ピークの詳細</h5>\"\n",
    "    html_content += f\"<div class='table-wrapper'>{top10_peak_table_html}</div>\"\n",
    "    html_content += \"<div class='flex-container'>\"\n",
    "    html_content += f\"<div class='flex-item'>{image_to_base64_html(os.path.join(output_dir, 'PCA_Top10_Score.png'), max_width='95%')} <p style='font-size:12px; text-align:center;'><b>スコアプロット:</b> サンプルの分布</p></div>\"\n",
    "    html_content += f\"<div class='flex-item'>{image_to_base64_html(os.path.join(output_dir, 'PCA_Top10_Loading.png'), max_width='95%')} <p style='font-size:12px; text-align:center;'><b>ローディングプロット:</b> ピークの寄与</p></div>\"\n",
    "    html_content += f\"<div class='flex-item'>{image_to_base64_html(os.path.join(output_dir, 'PLSDA_Top10_Score.png'), max_width='95%')} <p style='font-size:12px; text-align:center;'><b>スコアプロット:</b> サンプルの分布</p></div>\"\n",
    "    html_content += f\"<div class='flex-item'>{image_to_base64_html(os.path.join(output_dir, 'PLSDA_Top10_Loading.png'), max_width='95%')} <p style='font-size:12px; text-align:center;'><b>ローディングプロット:</b> ピークの寄与</p></div>\"\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>教師なしの手法（PCA, UMAP, t-SNE, デンドログラム）および教師ありの手法（PLS-DA, LDA）のいずれにおいても、各グループはある程度分離したクラスターを形成しています。特に、グループ情報を利用するPLS-DAとLDAにおいて分離が明瞭であることから、各グループを特徴づける化学的プロファイルが明確に存在することが強く示唆されます。PCAでも分離傾向が見られることは、グループ間の違いがデータ全体の変動に大きく寄与していることを示しています。デンドログラムは、各グループの類似度関係を示しており、どのグループが化学的に近いプロファイルを持つかを視覚的に示唆しています。<b>さらに、総合ランク上位10ピークのみを用いた再解析でも明確なグループ分離が確認できることから、これらの少数のピークがグループ間の違いを説明する上で極めて高い情報量を持つことが検証されました。</b></p></div>\"\n",
    "    \n",
    "    html_content += \"<h2>2.3. 分散分析（ANOVA）による有意ピークの探索</h2>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の目的</h3><p>グループ間の違いに貢献している可能性のあるピークを、統計的な手法を用いて網羅的にリストアップします。これは、<b>数千の候補の中から「何か意味がありそうな候補」を公平な基準で絞り込むための、最初のスクリーニング作業</b>です。</p><h3>解析のメカニズム</h3><p>全てのピークそれぞれに対して、グループを要因とする一元配置分散分析（ANOVA）を実行します。これにより得られる<b>p値</b>は、「全てのグループの平均値に本当は差がない」という仮説の下で、観測されたようなグループ間差が偶然生じる確率を示します。<br>しかし、数千回も検定（コイン投げなど）を繰り返すと、「偶然」大当たり（p値が小さい）が出てしまうことが増えます。この問題を制御するため、p値を補正し、<b>q値</b>（偽発見率）を算出します。</p><h3>解析の見方</h3><p>リストは、グループ間の差が統計的に最も信頼できる順（p値の昇順）にソートされています。</p><ul><li><b>p値 (p-value):</b> <b>一言で言うと「偶然その差が生まれる確率」</b>。値が小さいほど、その差が偶然ではなく、意味のある差である可能性が高まります。一般的に0.05（5%）が有意水準として用いられます。</li><li><b>q値 (q-value / FDR):</b> <b>一言で言うと「p値の信頼性スコア」</b>。多数の検定を行った後の補正版p値です。例えば「q値 < 0.05」を満たすピークを全て選んだ場合、そのリストに含まれるピークのうち、実際には差がない（選び間違い）ピークの割合が平均して5%程度に抑えられることを示します。<b>p値よりも厳しい基準であり、q値が小さいほど信頼性が高いと言えます。</b></li></ul><p class='highlight-box'><b>（具体的な解釈例）</b>もし「ピークA」のp値が 0.001、q値が 0.03 であった場合、「ピークAはグループ間で統計的に非常に有意な差があり（偶然その差が生まれる確率は0.1%）、かつ、その判断は多数の検定を経てもなお信頼できる（選び間違いの可能性が低い）」と解釈でき、強力なマーカー候補であると判断できます。</p></div>\"\n",
    "    if 'anova_results_sorted' in locals():\n",
    "        html_content += \"<h3>解析の結果の図表：ANOVA結果トップ20</h3><div class='table-wrapper'>\" + anova_results_sorted.head(20).to_html(classes='table table-striped') + \"</div>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>表の上位に示されるピーク群は、グループ間で統計的に信頼性の高い差が存在することを示しています。q値が有意水準（例: 0.05）を下回るピークは、グループ間の違いを反映するマーカー候補として特に有望です。ただし、この結果はあくまで「いずれかのグループ間に差がある」ことを示すのみです。どのグループとどのグループの差なのか、また分類への貢献度がどれだけ高いかは、後続の分析でさらに検証する必要があります。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h2>2.4. 重要特徴量の特定</h2>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の目的</h3><p>グループ間の違いを説明する上で最も重要な特徴量（ピーク）を特定します。ここでは、「統計的な有意差」と「機械学習モデルによる分類貢献度」という2つの異なるアプローチを統合し、より頑健なバイオマーカー候補をランキングします。</p><h3>解析のメカニズム</h3><p><b>料理に例えるなら、ANOVAは「味の違いに影響を与えた可能性のある食材」をリストアップする作業、ランダムフォレストは「どの食材が料理全体の味の決め手になったか」を評価するシェフのようなものです。</b>この2つの視点を統合することで、より本質的な要因を特定します。</p><ol><li><b>統計的有意差 (p値):</b> 「2.3. 分散分析（ANOVA）」で計算されたp値。グループ間の平均値の差が統計的にどれだけ信頼できるかを示します。</li><li><b>分類への重要度 (Feature Importance):</b> 機械学習モデルの一つであるランダムフォレストを構築し、各ピークがグループの分類（予測）にどれだけ貢献したかを数値化します。モデルが分類ルールを作成する際に、そのピークを頻繁に、かつ効果的に使用したほど、重要度は高くなります。</li></ol><p><b>総合ランク</b>は、p値の昇順ランクと重要度の降順ランクを足し合わせた値に基づき、その値が小さいほど上位（より重要）としています。</p><h3>解析の見方</h3><p><b>棒グラフ:</b> ランダムフォレストモデルが算出した「重要度」を上位20ピークについて可視化したものです。棒が長いほど、AIがグループ分類の際にそのピークを重視したことを示します。<br><b>表:</b> 統計的指標（p値, q値）と機械学習指標（重要度）を統合した総合ランキングです。</p><p class='highlight-box'><b>（具体的な解釈例）</b>もし「ピークX」が総合ランク1位であった場合、それは「ピークXは、統計的（ANOVA）に見てもグループ間の差が極めて明瞭であり、かつ、機械学習モデル（AI）がグループを分類する際にも最も重要な判断基準として利用した」ことを意味し、最も強力で信頼性の高いバイオマーカー候補であると解釈できます。</p></div>\"\n",
    "    html_content += \"<h3>解析の結果の図表</h3>\"\n",
    "    html_content += f\"<div class='center'>{image_to_base64_html(os.path.join(output_dir, '重要成分ランキング.png'))}</div>\"\n",
    "    if 'final_results' in locals():\n",
    "        html_content += \"<h3>表: 統合ランクによる重要特徴量トップ10</h3><div class='table-wrapper'>\" + final_results.head(10).to_html(classes='table table-striped') + \"</div>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>最も重要と判断されたピークは「<b>\" + (final_results.head(1)['ピーク名'].iloc[0] if 'final_results' in locals() and not final_results.empty else 'N/A') + \"</b>」でした。このピークを含む上位の成分が、各グループの化学的特徴を決定づけている主要因と考えられます。棒グラフと表の上位に共通して現れるピークは、特に注目に値します。これらのピークの変動パターンを追跡することが、グループの特性を理解する鍵となります。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h2>2.5. バイオマーカー候補の検証（最重要特徴量）</h2>\" \n",
    "    html_content += \"<div class='interpretation'><h3>解析の目的</h3><p>「2.4. 重要特徴量の特定」で同定された総合ランク上位のピークが、実際にグループ間でどのような値の分布をしているかを視覚的に確認し、その差のパターン（どのグループで高い/低いか）を検証します。<b>統計やAIが「重要だ」と判断したものが、本当に見た目にも分かりやすい差なのかを自分の目で確かめるセクションです。</b></p><h3>解析のメカニズム</h3><p>総合ランク上位4つのピークについて、グループごとにバイオリンプロットを作成します。バイオリンプロットは、データの分布密度（形状の膨らみ）と四分位数（内部の箱ひげ図）を同時に示すグラフです。</p><h3>解析の見方</h3><p>各プロットは、一つのピークに対するグループごとの値の分布を示しています。</p><ul><li><b>バイオリンの形状:</b> 膨らんでいる箇所にデータが密集していることを示します。幅が広いほど、その値を持つサンプルが多いことを意味します。</li><li><b>バイオリンの位置:</b> 特定のグループのバイオリン全体が、他のグループよりも明確に高い、あるいは低い位置にあるかどうかに注目します。</li><li><b>重なり:</b> グループ間のバイオリンの重なりが少ないほど、そのピークによるグループの分離が明瞭であることを示します。</li></ul><p class='highlight-box'><b>（具体的な解釈例）</b>もし「ピークY」のプロットで、A群のバイオリンがB群やC群よりも明らかに高い位置にあり、重なりも少なければ、「ピークYはA群において特異的に高値を示す傾向がある」と解釈でき、A群のマーカー候補として有望であると判断できます。</p></div>\"\n",
    "    html_content += \"<h3>解析の結果の図表</h3>\"\n",
    "    html_content += f\"<div class='center'>{image_to_base64_html(os.path.join(output_dir, '最重要特徴量バイオリンプロット.png'))}</div>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>プロットされた4つのピークは、総合ランク上位であることに違わず、視覚的にもグループ間で明確な分布の違いを示しています。例えば、特定のピークがあるグループで一貫して高い値を示したり、別のグループで低い値を示したりする傾向が確認できます。このように、統計的・機械学習的に重要とされたピークが、実際のデータ分布においてもグループ差を裏付けていることは、これらのピークがバイオマーカーとして機能する可能性が高いことを強く支持しています。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h2>2.6. 各グループのトップピーク可視化</h2>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の目的</h3><p>各グループの「化学的な個性」を明確にするため、他の全てのグループと比較して、そのグループで特異的に量が多い（または少ない）ピークを同定し、可視化します。<b>「グループAを特徴づけているのは、どのような点ですか？」という問いに答えるのがこのセクションです。</b></p><h3>解析のメカニズム</h3><p>各グループ（例: A群）について、「A群の各ピークの平均値」と「A群以外の全サンプルの各ピークの平均値」を計算します。次に、その比率（Fold Change）を算出します。<br><div class=\\\\\\\"formula\\\\\\\" style=\\\\\\\"font-size:1.1em; margin:10px 0;\\\\\\\">$$ \\\\text{Fold Change} = \\\\frac{\\\\text{対象グループの平均値}}{\\\\text{それ以外の全グループの平均値}} $$</div>このFold Changeが大きかった（= 対象グループで特に量が多かった）ピークを、重要特徴量（総合ランク上位30位）の中から抽出し、上位5つを棒グラフで表示します。</p><h3>解析の見方</h3><p>各グラフは、そのタイトルに示されたグループ（例: 「A群」）を特徴づける（A群で特に量が多い）上位5つのピークを示しています。横軸の数値（Fold Change）が大きいほど、他のグループ全体と比較して顕著に量が多いことを意味します。</p><p class='highlight-box'><b>（具体的な解釈例）</b>「グループA」のグラフで「ピークZ」がFold Change 5.0で1位であった場合、「ピークZは、A群において他のグループ全体の平均よりも約5倍多く存在する、A群の強力な特徴マーカーである」と解釈できます。</p></div>\"\n",
    "    html_content += \"<h3>解析の結果の図表</h3><div class='flex-container'>\"\n",
    "    for group_name in ordered_groups_for_plotting:\n",
    "        filepath = os.path.join(output_dir, f'トップピーク_{group_name}.png')\n",
    "        html_content += f\"<div class='flex-item'>{image_to_base64_html(filepath, max_width='100%')}</div>\"\n",
    "    html_content += \"</div>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>これらのグラフは、各グループの「顔」となる化学的プロファイルを示しています。各グループで特異的に量の多いピークが異なることから、それぞれのグループが独自の化学的特徴を持つことがわかります。これらの情報は、各グループの生物学的な状態や特性を解釈する上で有用な手がかりとなります。また、「2.4. 重要特徴量の特定」で上位だったピークがここでも現れる場合、そのピークが全体の違いと特定のグループの識別の両方に寄与していることがわかります。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h2>2.7. 分類モデルの性能評価</h2>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の目的</h3><p>サンプルの全ピークプロファイルを用いて、そのサンプルがどのグループに属するかを自動で分類する機械学習モデル（AI）を構築します。その上で、モデルが未知のデータに対しても正しく分類できるか（汎化性能）を客観的に評価し、さらにモデルが学習した分類ロジック（判断基準）を可視化して解釈します。</p><h3>解析のメカニズム</h3><ul><li><b>交差検証 (Cross-Validation):</b> モデルの「本当の実力」を評価する手法です。<b>例えるなら、データを複数回分の「模擬試験」に分け、モデルに解かせて平均点を出すようなものです。</b>これにより、未知のデータに対する性能（汎化性能）を信頼性高く推定します。</li><li><b>混同行列 (Confusion Matrix):</b> モデルの「間違い方」の癖を示す成績表です。縦軸に「実際のグループ」、横軸に「モデルの予測結果」を配置します。</li><li><b>ROC曲線:</b> 各グループを「その他」から区別する際のモデルの識別能力を視覚化します。<b>AUC（曲線下の面積）</b>が1.0に近いほど高性能（1.0なら完璧）です。</li><li><b>決定木:</b> AI（ランダムフォレスト）の複雑な思考プロセスを、人間が理解しやすい単純な「もし〜なら〜」の分岐ルールとして可視化したものです。</li></ul><h3>解析の見方</h3><ul><li><b>モデル性能サマリー:</b> 交差検証による平均正解率を示します。100%に近いほど、未知のデータに対しても高い精度が期待できます。</li><li><b>混同行列（左図）:</b> 対角線上の数値が「正解数」です。対角線以外に数字があれば誤分類を意味し、モデルがどのグループ間を混同しやすいかがわかります。</li><li><b>ROC曲線（右図）:</b> 曲線が左上の角にピッタリと近づくほど、そのグループを識別する性能が高いことを示します。</li><li><b>決定木:</b> 最も上のノード（根）が、分類においてAIが最初にチェックする最も重要な分岐条件を示します。</li></ul><p class='highlight-box'><b>（具体的な解釈例）</b>交差検証の平均正解率が 98% であれば、「このモデルは、未知のサンプルが来た場合でも、約98%の確率で正しくグループを分類できる」と期待できます。混同行列でA群（実際）とB群（予測）の交差マスに「1」とあれば、「実際はA群のサンプルが1つ、B群であると誤って予測された」と読み取れます。</p></div>\"\n",
    "    if 'cv_accuracy' in locals():\n",
    "        html_content += f\"<div class='summary'><p><b>モデル性能サマリー:</b> 交差検証による推定精度は <b>{cv_accuracy:.1%}</b> となりました。これは、各グループが化学的プロファイルに基づいて高い信頼性で分類可能であることを示しています。</p></div>\"\n",
    "    html_content += \"<h3>解析の結果の図表</h3>\"\n",
    "    html_content += f\"<div class='center'>{image_to_base64_html(os.path.join(output_dir, 'モデル性能評価.png'))}</div>\"\n",
    "    if 'report_df' in locals():\n",
    "        html_content += \"<h3>表: 分類精度レポート</h3><div class='table-wrapper'>\" + report_df.to_html(classes='table table-striped') + \"</div>\"\n",
    "    html_content += f\"<div class='center'>{image_to_base64_html(os.path.join(output_dir, '決定木.png'))}</div>\"\n",
    "    if 'cv_accuracy' in locals():\n",
    "        html_content += f\"<div class='interpretation'><h3>解析の結果と考察</h3><p>交差検証の結果 ({cv_accuracy:.1%}) は、本データセットがグループ間で化学的に明確に分離可能であり、未知のデータに対しても高い予測性能を持つモデルが構築可能であることを強く示唆しています。混同行列および分類精度レポートは、学習データに対する再分類精度を示しており、ほぼ全てのサンプルが正しく分類されています。決定木からは、分類の最初の鍵となっているピークが読み取れ、これは「2.4. 重要特徴量の特定」の結果とも整合しています。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h1>3. 補足分析</h1>\"\n",
    "    html_content += \"<h2>3.1. グループの距離分析</h2>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の目的</h3><p>各グループの「均一性（グループ内のまとまり具合）」と、グループ間の「類似性（グループ同士の離れ具合）」を、客観的な距離尺度を用いて定量化し、比較します。</p><h3>解析のメカニズム</h3><p>全てのピーク（特徴量）を標準化した $n$ 次元空間において、サンプル間のユークリッド距離を用いて以下の2つの指標を計算します。</p><ul><li><b>内部距離（左図）:</b> 各グループに属するサンプル同士の全てのペア間のユークリッド距離を計算し、その平均値を算出します。この値が小さいほど、グループ内のサンプル同士が互いに似ている（<b>個性が少なく、均一である</b>）ことを意味します。</li><li><b>グループ間距離（右図）:</b> 各グループの全サンプルの平均プロファイル（重心）を計算します。その後、グループの重心間のユークリッド距離を総当たりで計算します。この値が大きいほど、グループ間の平均的なプロファイルが異なっている（<b>化学的に遠い関係にある</b>）ことを意味します。</li></ul><div class=\\\\\\\"formula\\\\\\\">$$ d(\\\\mathbf{p}, \\\\mathbf{q}) = \\\\sqrt{\\\\sum_{i=1}^{n} (p_i - q_i)^2} $$</div><h3>解析の見方</h3><p><b>内部距離（棒グラフ）:</b> 棒が低いほど、そのグループは均一性が高い（まとまりが良い）ことを示します。<br><b>グループ間距離（ヒートマップ）:</b> 色が明るい（値が大きい）ペアほど、グループ間の化学的特徴が大きく異なる（離れている）ことを示します。対角成分は0（自分自身との距離）です。</p><p class='highlight-box'><b>（具体的な解釈例）</b>もし「A群」の内部距離が 3.0、「B群」が 6.0 であれば、「A群はB群と比較して、サンプル間のばらつきが小さく、2倍均一な集団である」と解釈できます。もしヒートマップで「A-B間」の距離が 10.0、「A-C間」の距離が 5.0 であれば、「A群はC群よりもB群との方が化学的特徴が大きく異なる」と解釈できます。</p></div>\"\n",
    "    html_content += \"<h3>解析の結果の図表</h3>\"\n",
    "    html_content += f\"<div class='center'>{image_to_base64_html(os.path.join(output_dir, '内部距離.png'), max_width='48%')} {image_to_base64_html(os.path.join(output_dir, 'グループ間距離.png'), max_width='48%')}</div>\"\n",
    "    if 'internal_distances' in locals() and 'inter_group_distances' in locals():\n",
    "        min_internal_dist_group, max_internal_dist_group = min(internal_distances, key=internal_distances.get), max(internal_distances, key=internal_distances.get)\n",
    "        max_inter_dist_pair_tuple = max(inter_group_distances, key=inter_group_distances.get)\n",
    "        max_inter_dist_pair_str = f\"{max_inter_dist_pair_tuple[0]} vs {max_inter_dist_pair_tuple[1]}\"\n",
    "        html_content += f\"<div class='interpretation'><h3>解析の結果と考察</h3><ul><li><b>まとまり具合:</b> グループ <b>'{min_internal_dist_group}'</b> が最も均一な集団（内部距離: {internal_distances[min_internal_dist_group]:.2f}）であり、<b>'{max_internal_dist_group}'</b> が最も多様な（ばらつきの大きい）集団（内部距離: {internal_distances[max_internal_dist_group]:.2f}）です。</li><li><b>離れ具合:</b> ペア <b>'{max_inter_dist_pair_str}'</b> が最も化学的プロファイルが異なる組み合わせ（グループ間距離: {inter_group_distances[max_inter_dist_pair_tuple]:.2f}）です。</li></ul><p>これらの距離関係は、「2.2. 全体構造の可視化と主要ピークの特定」で示された各手法のプロットにおけるクラスターのまとまり具合や、クラスター間の距離感とも概ね一致しています。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h2>3.2. ボルケーノプロットによるマーカー探索</h2>\"\n",
    "    html_content += r'<div class=\"interpretation\"><h3>解析の目的</h3><p>2つのグループ間を直接比較し、「統計的な信頼性（p値）」と「変動の大きさ（Fold Change）」の両方を同時に満たす、グループ間差に本質的なピーク（マーカー候補）を効率的に探索します。<b>火山が噴火したような見た目から、ボルケーノ（火山）プロットと呼ばれます。</b></p><h3>解析のメカニズム</h3><p>指定された2グループ（例: A群 vs B群）に属するサンプルのみを使用し、全てのピークについて以下の2つの指標を計算して2次元にプロットします。</p><ul><li><b>横軸 (Log2 Fold Change):</b> 2つのグループ間での各成分の<b>「量的な変動の大きさ」</b>を示します。A群の平均値をB群の平均値で割った値の対数（底2）です。<div class=\"formula\" style=\"font-size:1.1em; margin:10px 0;\">$$ \\text{横軸} = \\log_2 \\left( \\frac{\\text{グループAの各ピークの平均値}}{\\text{グループBの各ピークの平均値}} \\right) $$</div></li><li><b>縦軸 (-Log10 p-value):</b> 2つのグループ間の差が<b>「統計的にどれだけ信頼できるか」</b>を示します。p値は対応のないt検定（Welch\\'s t-test）により算出します。p値が小さいほど縦軸の値は大きくなります。<div class=\"formula\" style=\"font-size:1.1em; margin:10px 0;\">$$ \\text{縦軸} = -\\log_{10}(p\\text{-value}) $$</div></li></ul><h3>解析の見方</h3><ul><li><b>横軸:</b> 0から離れるほど（右に行くほどA群で多く、左に行くほどB群で多い）、2群間の量的な差が大きいことを示します。一般的に \\( \\left| \\log_2(\\text{Fold Change}) \\right| > 1 \\)（すなわち2倍以上の変動）が注目されます。</li><li><b>縦軸:</b> 上に行くほどp値が小さく、その差が偶然である可能性が低い（統計的に信頼できる）ことを示します。一般的に \\( p < 0.05 \\)（すなわち \\( -\\log_{10}(0.05) \\approx 1.3 \\)）より上の領域が注目されます。</li><li><b>右上・左上:</b> プロットの<b>右上の領域</b>（A群で多く、かつ有意）および<b>左上の領域</b>（B群で多く、かつ有意）に位置する点が、両群を分ける重要なマーカー候補、つまり<b>噴火している有望なピーク</b>です。</li></ul><details style=\"margin-top: 15px; padding: 10px; background-color: #f0f0f0; border-radius: 5px;\"><summary style=\"cursor: pointer; font-weight: bold;\">【補足】ニアリーイコール（≈）とは？</summary><div style=\"margin-top: 10px; padding-top: 10px; border-top: 1px solid #ccc;\"><p style=\"margin:0;\">この `≈` 記号は「ニアリーイコール」と読み、<b>「ほぼ等しい」</b>を意味します。<br>例えば、`\\( -\\log_{10}(0.05) \\)` の正確な計算結果は `1.3010...` という長い小数ですが、分かりやすくするために「約1.3」として丸めた値で表記しています。`=` が「完全に等しい」ことを示すのに対し、`≈` は「完全ではないが、実用上ほぼ等しい」ことを示す記号です。</p></div></details><p class=\"highlight-box\"><b>（具体的な解釈例）</b>「A群 vs B群」のプロットで、「ピークP」が右上の領域（例: 横軸 > 1, 縦軸 > 1.3）にプロットされた場合、「ピークPは、A群においてB群よりも2倍以上多く、その差は統計的にも有意（\\( p<0.05 \\)）である」と解釈できます。</p></div>'\n",
    "    html_content += \"<h3>解析の結果の図表</h3><div class='flex-container'>\"\n",
    "    volcano_files = sorted(glob.glob(os.path.join(output_dir, 'ボルケーノプロット_*.png')))\n",
    "    for filepath in volcano_files:\n",
    "        html_content += f\"<div class='flex-item'>{image_to_base64_html(filepath, max_width='100%')}</div>\"\n",
    "    html_content += \"</div>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>各ペアの比較プロットは、それぞれのグループペア間でのみ特異的に変動するピークを明らかにします。これにより、特定の2群間比較における特異的なマーカーを探索することができます。「2.3. 分散分析（ANOVA）」が全グループを対象とするのに対し、ボルケーノプロットは特定の2群間に焦点を当てた詳細なマーカー探索を可能にします。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h2>3.3. 重要特徴量の相関ヒートマップ</h2>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の目的</h3><p>「2.4. 重要特徴量の特定」で同定された上位のピーク同士が、互いにどのような連動性（相関関係）を持っているかを可視化します。これにより、マーカー候補群が独立しているのか、あるいは<b>共通のメカニズムによって連動して変動している「仲間」なのか</b>を考察する手がかりを得ます。</p><h3>解析のメカニズム</h3><p>総合ランク上位20ピークのデータのみを抽出し、総当たりの相関係数（ピアソンの積率相関係数）を計算し、ヒートマップとして可視化します。似たような相関パターンを持つピーク同士が近くに集まるように並び替えられています（クラスタリング）。</p><h3>解析の見方</h3><ul><li><b>色の解釈:</b> 色が<b>赤色</b>に近いほど、2つのピーク間に強い<b>正の相関</b>（一方が増加すると、他方も増加する傾向）があることを示します。色が<b>青色</b>に近いほど、強い<b>負の相関</b>（一方が増加すると、他方は減少する傾向）があることを示します。</li><li><b>クラスター:</b> ヒートマップ上で赤色や青色のブロックを形成している場合、それらが一群の連動する因子群である可能性を示唆します。</li></ul><p class='highlight-box'><b>（具体的な解釈例）</b>もし「ピークA」と「ピークB」の交差するマスが濃い赤色であれば、「ピークAとピークBは、サンプル内で同様の増減パターンを示す」と解釈できます。これは、これらが例えば共通の代謝経路に属している、あるいは共通の上流因子によって制御されている可能性を示唆します。</p></div>\"\n",
    "    html_content += \"<h3>解析の結果の図表</h3>\"\n",
    "    html_content += f\"<div class='center'>{image_to_base64_html(os.path.join(output_dir, '相関ヒートマップ.png'))}</div>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>ヒートマップから、重要ピーク間にいくつかの相関グループが見られます。例えば、互いに強い正の相関を示すピーク群は、一群の連動する因子群を形成している可能性が示唆されます。一方、負の相関関係にあるピーク群は、拮抗的な変動パターンを持つ可能性があります。このような相関情報は、マーカー候補を絞り込む際（例：相関の強い群からは代表を1つ選ぶ）や、変動の背後にある生物学的メカニズムを考察する上で重要です。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h2>3.4. SHAPによるモデル解釈</h2>\"\n",
    "    html_content += \"\"\"\n",
    "    <div class='interpretation'>\n",
    "        <h3>解析の目的</h3>\n",
    "        <p>機械学習モデル（ランダムフォレスト）が、なぜ特定のグループ（例: A群）であると予測したのか、その「判断の根拠」をピーク単位で詳細に可視化します。これにより、AIの予測プロセス（ブラックボックス）の透明性を高め、モデルが妥当な理由に基づいて判断しているかを確認します。<b>例えるなら、優秀なシェフに「なぜこの料理が美味しいのか」の秘訣を教えてもらうようなものです。</b></p>\n",
    "        <h3>解析のメカニズム</h3>\n",
    "        <p>SHAP (SHapley Additive exPlanations) と呼ばれる、協力ゲーム理論に基づく手法を用います。各サンプル・各ピークについて、「そのピークの値が、モデルの最終予測（例: A群である確率）を、平均的な予測値からどれだけ押し上げたか（正の貢献）、あるいは押し下げたか（負の貢献）」を「<b>SHAP値</b>」として公平に算出します。</p>\n",
    "        <h3>解析の見方</h3>\n",
    "        <h4>1. サマリープロット (全体サマリー)</h4>\n",
    "        <p>このグラフは、特定のグループ（例: \"A群\"）の予測に対して、<strong>どのピークが全体的に重要であったか</strong>、そして<strong>そのピークの値が予測にどう影響したか</strong>を示します。</p>\n",
    "        <ul style=\"list-style-type: disc; margin-left: 20px;\">\n",
    "            <li><b>縦軸 (特徴量):</b> 予測への平均的な影響度が大きかった順に、上から並んでいます。</li>\n",
    "            <li><b>横軸 (SHAP値):</b> 予測への貢献度を示します。正の値（右側）はそのグループであるという予測を<strong>強める方向</strong>、負の値（左側）はそのグループではないという予測を<strong>強める方向</strong>に働いたことを示します。</li>\n",
    "            <li><b>点の色 (特徴量の値):</b> そのサンプルにおけるピークの値の大小を示します（赤色: 高い, 青色: 低い）。</li>\n",
    "        </ul>\n",
    "        <p class='highlight-box'>\n",
    "            <b>(解釈例)</b> 「グループA」のサマリープロットで、「ピークX」の行に<strong>赤い点 (高値)</strong> が主に<strong>右側 (正のSHAP値)</strong> に分布している場合、<strong>「モデルは、ピークXの値が高いほどA群であると強く判断している」</strong>と明確に解釈できます。\n",
    "        </p>\n",
    "        <h4>2. ディシジョンプロット (個別サンプル解析)</h4>\n",
    "        <p>このグラフは、<strong>たった1つのサンプル</strong>を取り上げ、そのサンプルの予測がどのように決定されたかを詳細に示す「意思決定のプロセス」の可視化です。</p>\n",
    "        <ul style=\"list-style-type: disc; margin-left: 20px;\">\n",
    "            <li>一番下の値 (Base Value) から始まり、各ピークの寄与（矢印）を経て、一番上の最終予測スコア (Model Output) に至る経路が、予測の根拠となります。</li>\n",
    "            <li>右向きの赤い矢印は予測スコアを押し上げる要因、左向きの青い矢印は押し下げる要因です。</li>\n",
    "        </ul>\n",
    "        <p class='highlight-box'>\n",
    "            <b>(解釈例)</b> 「A群」に属する「Sample-01」のプロットで、Base Valueから始まり、「ピークX」が高値だったためにスコアが右に大きく移動し、最終的なModel Outputが高い値に達した場合、<strong>「このサンプルは、主にピークXが高かったためにA群であると強く予測された」</strong>と解釈できます。\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    html_content += \"<h3>解析の結果の図表 (1/2：全体サマリー)</h3><div class='flex-container'>\"\n",
    "    for i, class_name in enumerate(model.classes_):\n",
    "        filepath = os.path.join(output_dir, f'SHAP_Summary_{class_name}.png')\n",
    "        html_content += f\"<div class='flex-item'>{image_to_base64_html(filepath, max_width='100%')}</div>\"\n",
    "    html_content += \"</div>\"\n",
    "    html_content += \"<h3>解析の結果の図表 (2/2：個別サンプル解析)</h3><div class='flex-container'>\"\n",
    "    decision_plot_files = sorted(glob.glob(os.path.join(output_dir, 'SHAP_Decision_*.png')))\n",
    "    if decision_plot_files:\n",
    "        html_content += \"<p style='width:100%; text-align:center;'>（各グループの代表的なサンプルについて、予測の判断根拠を表示しています）</p>\"\n",
    "        for filepath in decision_plot_files:\n",
    "            html_content += f\"<div class='flex-item'>{image_to_base64_html(filepath, max_width='100%')}</div>\"\n",
    "    else:\n",
    "        html_content += \"<p>個別サンプルの解析図表は生成されませんでした。</p>\"\n",
    "    html_content += \"</div>\"\n",
    "    html_content += \"<div class='interpretation'><h3>解析の結果と考察</h3><p>SHAPサマリープロットは、「2.4. 重要特徴量の特定」で示された重要ピークが、実際にモデルの予測にどのように貢献しているかを明確に示しています。例えば、特定のピークが高い値（赤色）のときにSHAP値が正になる場合、そのピークがそのグループの陽性マーカーとして機能していることがわかります。ディシジョンプロットは、個々のサンプルの予測におけるこれらのマーカーの役割を具体的に示し、モデルの予測に対する信頼性を高めます。これらの結果は、AIがデータに基づいた妥当な判断を行っていることを裏付けています。</p></div>\"\n",
    "    \n",
    "    html_content += \"<h1>4. AIによる統合考察</h1>\"\n",
    "    if 'ai_interpretation_text' not in locals() or not ai_interpretation_text:\n",
    "        print(\"   > 警告: 前のセルで生成されたAI考察('ai_interpretation_text')が見つかりません。簡易版を生成します。\")\n",
    "        ai_interpretation_text = generate_ai_interpretation()\n",
    "    html_content += f\"<div class='summary'>{ai_interpretation_text}</div>\"\n",
    "    \n",
    "    html_content += \"<h1>5. 解析環境</h1>\"\n",
    "    html_content += env_info_html # ★★★ 追加 ★★★\n",
    "    \n",
    "    html_content += \"</body></html>\"\n",
    "    report_filepath_html = os.path.join(output_dir, '⭐︎ Heracles NEO 総合解析レポート.html')\n",
    "    with open(report_filepath_html, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    print(f\"✅ 機能を追加・改善した詳細なHTMLレポートが {report_filepath_html} に正常に出力されました。\")\n",
    "except Exception as e:\n",
    "    print(f\"   > ❌ ERROR: HTMLレポートの生成中にエラーが発生しました: {e}\")\n",
    "    print(traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
